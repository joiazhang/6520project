#' @param lr A constant that is the learning rate.
#' @param beta_0 An p x 1 vector that is the initialization for the coefficients.
#' @param full Boolean, if true uses the full G matrix for the update step, otherwise uses only the diagonal elements of G.
#' @param regression Boolean, regression if true, else classification.
#' @return List where first elemnt is an n x p matrix where each ith row is the coefficients for the ith iteration and the columns are predictors and second element is a nx1 vector of runtimes for each iteration.
#' @examples
#' my_adagrad(X=X, Y=Y, lr=0.00001, beta_0=rep(0, ncol(X)), full=T)
#' my_adagrad(X=X, Y=Y, lr=0.00001, beta_0=runif(ncol(X)), full=F)
#' @export
my_adagrad = function(X, Y, lr, beta_0, full, regression=T) {
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
g_vec = matrix(nrow=n, ncol=p) # save matrix for the gradients where each gradient g_t is the t^{th} row of the matrix
G_t = matrix(data=rep(0, p^2), nrow=p, ncol=p) # matrix that is a cumulative sum
runtimes = rep(NA, n)
runtimes[1] = 0
if (regression) {
for (t in 1:(n-1)) {
start_time = Sys.time()
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
g_vec[t, ] = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
g_t = as.matrix(g_vec[t, ])
G_t = G_t + g_t%*%t(g_t)
diag_G_t = diag(diag(G_t), nrow=p, ncol=p)
if (full) {
# full
# betahats[t+1, ] = beta_t - lr*as.matrix(solve(as.matrix(expm::sqrtm(G_t))))%*%g_t # TODO: there is an issue here
} else {
# diagonal
betahats[t+1, ] = beta_t - lr*as.matrix(diag(diag(diag_G_t^(-1/2)), nrow=p, ncol=p))%*%g_t
}
end_time = Sys.time()
runtimes[t+1] = runtimes[t] + (end_time - start_time)
} # end for
} else {
# classification
for (t in 1:(n-1)) {
start_time = Sys.time()
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
# predict y
Z = t(beta_t)%*%x_t # omit intercept bs
Y_pred = 1/(1+1/exp(Z))
g_vec[t, ] = (1/n)*as.numeric(Y_pred-Y_t)*x_t
g_t = as.matrix(g_vec[t, ])
G_t = G_t + g_t%*%t(g_t)
diag_G_t = diag(diag(G_t), nrow=p, ncol=p)
betahats[t+1, ] = beta_t - lr*as.matrix(diag(diag(diag_G_t^(-1/2)), nrow=p, ncol=p))%*%g_t # diagonal update step
end_time = Sys.time()
runtimes[t+1] = runtimes[t] + (end_time - start_time)
}
}
return(list(betahats, runtimes))
}
set.seed(6520)
library(ggplot2)
library(expm) # for sqrtm
library(devtools)
library(roxygen2)
# plot prediction or estimation error
# X: rows are observations, columns are predictors
# Y: response variable
# betahats: n x p matrix where each ith row is the coefficients for the ith iteration and the columns are predictors
# beta: true beta coefficient px1 vector
# title: string for the title of the plot
# type: "prediction" or "estimation" for prediction error or estimation error
plot_prediction_error = function(betahats, beta, X, Y, title, type, regression=T, bs) {
n = nrow(X)
p = ncol(X)
if ((type!="prediction") && (type!="estimation")) {
stop("type parameter must be 'prediction' or 'estimation'")
}
if (type=="prediction") {
if (regression) {
# prediction error
err = colSums((X%*%t(betahats) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))^2) # row of the inside matrix is observation, column is iteration
ylab = "Prediction error"
} else {
# classification
# predict y
Z = as.matrix(rowSums(betahats*X))+bs
Y_pred = 1/(1+1/exp(Z))
Yhat = rep(NA, n)
Yhat[Y_pred < 0.5] = 0
Yhat[Y_pred >= 0.5] = 1
# predict y
err = cumsum(Yhat!=Y)/(1:n)
ylab = "Misclassification rate"
}
} else {
# estimation error
beta = t(beta)
beta = matrix(rep(beta, n),nrow=n, byrow=T) # row combine n number of t(beta)'s
err = sqrt(rowSums((betahats - beta)^2))
ylab = "Estimation error"
}
err = as.matrix(err)
plot(err, xlab="Iteration", ylab=ylab, main=title)
return(err)
}
# plot last iteration of betahat for nonzero vs zero indexes, true beta overlaid
# nonzero_indexes: kx1 vector of indexes where the k-sparse vector true beta has nonzero values
# nonzero: boolean, if true plot only nonzero indexes (k indexes of the true k-sparse beta), otherwise plot zero indexes
plot_betas = function(betahats, beta, nonzero_indexes, nonzero=T) {
n = nrow(betahats)
p = ncol(betahats)
dat = data.frame("p"=1:p, "bethat_n"=betahats[n, ], "beta"=beta)
if (nonzero) {
dat = dat[nonzero_indexes, ]
title = "Last iteration of bethat (orange) and true beta (black) at nonzero indexes"
} else {
dat = dat[-nonzero_indexes, ]
title = "Last iteration of bethat (orange) and true beta (black) at zero indexes"
}
ggplot(dat) + geom_point(aes(x=p, y=bethat_n), color="orange") +
geom_point(aes(x=p, y=beta), color="black", shape=4) +
xlab("p") +
ylab("") +
ggtitle(title)
}
# simulate data: regression
n = 100 # sample size
p = 200 # number of predictors
# beta
k = round(0.05*p, 0) # number of nonzero coefficients
sd_beta = 0.01
nonzero_indexes = sample.int(n=p, size=k)
beta = rep(0, p)
beta[nonzero_indexes] = rnorm(n=k, mean=100, sd=sd_beta)
sum(which(beta !=0) != sort(nonzero_indexes)) # test that we made the right indexes nonzero
beta = as.matrix(beta)
# x
X = matrix(rnorm(n=n*p, mean=0, sd=5), nrow=n)
# epsilon
E = matrix(rnorm(n=n, mean=0, sd=1), nrow=n)
# y
Y = X%*%beta + E
# note that in the online setting, each t^th row of X and Y is for time t
runtime_df = data.frame("n"=1:n)
pred_df = data.frame("n"=1:n) # prediction error
est_df = data.frame("n"=1:n) # estimation error
# OGD
temp = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p))
betahats = temp[[1]]
runtimes = temp[[2]]
runtime_df$ogd = runtimes
pred_df$ogd = plot_prediction_error(betahats, beta, X, Y, title="Online gradient descent (OGD)", type="prediction")
est_df$ogd = plot_prediction_error(betahats, beta, X, Y, title="Online gradient descent (OGD)", type="estimation")
plot_betas(betahats, beta, nonzero_indexes, nonzero=T) # nonzero indexes
plot_betas(betahats, beta, nonzero_indexes, nonzero=F) # zero indexes
# Adagrad
temp = my_adagrad(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p), full=F)
betahats = temp[[1]]
runtimes = temp[[2]]
runtime_df$adagrad = runtimes
pred_df$adagrad = plot_prediction_error(betahats, beta, X, Y, title="Adaptive gradient descent (Adagrad)", type="prediction")
est_df$adagrad = plot_prediction_error(betahats, beta, X, Y, title="Adaptive gradient descent (Adagrad)", type="estimation")
plot_betas(betahats, beta, nonzero_indexes, nonzero=T)
plot_betas(betahats, beta, nonzero_indexes, nonzero=F)
# Adagrad
temp = my_adagrad(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p), full=F)
betahats = temp[[1]]
runtimes = temp[[2]]
runtime_df$adagrad = runtimes
pred_df$adagrad = plot_prediction_error(betahats, beta, X, Y, title="Adaptive gradient descent (Adagrad)", type="prediction")
est_df$adagrad = plot_prediction_error(betahats, beta, X, Y, title="Adaptive gradient descent (Adagrad)", type="estimation")
plot_betas(betahats, beta, nonzero_indexes, nonzero=T)
plot_betas(betahats, beta, nonzero_indexes, nonzero=F)
# Adam
temp = my_adam(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p), rho_1=0.9, rho_2=0.999, epsilon=1e-8)
betahats = temp[[1]]
runtimes = temp[[2]]
runtime_df$adam = runtimes
pred_df$adam = plot_prediction_error(betahats, beta, X, Y, title="Adaptive moment estimation (Adam)", type="prediction")
est_df$adam = plot_prediction_error(betahats, beta, X, Y, title="Adaptive moment estimation (Adam)", type="estimation")
plot_betas(betahats, beta, nonzero_indexes, nonzero=T)
plot_betas(betahats, beta, nonzero_indexes, nonzero=F)
# plot prediction error for ogd, adagrad, adam
ggplot(pred_df, aes(x=n)) +
geom_line(aes(y=ogd), color="red") +
geom_line(aes(y=adagrad), color="blue") +
geom_line(aes(y=adam), linetype="dashed") +
xlab("Iteration") +
ylab("") +
ggtitle("Prediction error for OGD (red), Adagrad (blue), Adam (black)")
# plot estimation error for ogd, adagrad, adam
ggplot(est_df, aes(x=n)) +
geom_line(aes(y=ogd), color="red") +
geom_line(aes(y=adagrad), color="blue") +
geom_line(aes(y=adam), linetype="dashed") +
xlab("Iteration") +
ylab("") +
ggtitle("Estimation error for OGD (red), Adagrad (blue), Adam (black)")
# plot runtime for ogd, adagrad, adam
ggplot(runtime_df, aes(x=n)) +
geom_line(aes(y=ogd), color="red") +
geom_line(aes(y=adagrad), color="blue") +
geom_line(aes(y=adam), linetype="dashed") +
xlab("Iteration") +
ylab("") +
ggtitle("Runtime for OGD (red), Adagrad (blue), Adam (black)")
conv_df = data.frame("n"=1:n) # plot convergence
conv_df$err_ogd = pred_df$ogd
conv_df$err_adagrad = pred_df$adagrad
conv_df$err_adam = pred_df$adam
conv_df$time_ogd = runtime_df$ogd
conv_df$time_adagrad = runtime_df$adagrad
conv_df$time_adam = runtime_df$adam
# plot convergence for ogd, adagrad, adam
ggplot(conv_df) +
geom_line(aes(x=time_ogd, y=err_ogd), color="red") +
geom_line(aes(x=time_adagrad, y=err_adagrad), color="blue") +
geom_line(aes(x=time_adam, y=err_adam), linetype="dashed") +
xlab("Runtime") +
ylab("Prediction error") +
ggtitle("Convergence for OGD (red), Adagrad (blue), Adam (black)")
# simulate data: classification
# X, beta same as above
probs = 1/(1+exp(-X%*%beta))
Y = rbinom(n=n, size=1, prob = probs) # Bernoulli
runtime_df = data.frame("n"=1:n)
pred_df = data.frame("n"=1:n) # prediction error
est_df = data.frame("n"=1:n) # estimation error
# OGD
temp = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p), regression=F)
betahats = temp[[1]]
runtimes = temp[[2]]
bs = temp[[3]]
runtime_df$ogd = runtimes
pred_df$ogd = plot_prediction_error(betahats, beta, X, Y, title="Online gradient descent (OGD)", type="prediction", regression=F, bs=bs)
est_df$ogd = plot_prediction_error(betahats, beta, X, Y, title="Online gradient descent (OGD)", type="estimation")
plot_betas(betahats, beta, nonzero_indexes, nonzero=T) # nonzero indexes
plot_betas(betahats, beta, nonzero_indexes, nonzero=F) # zero indexes
# Adagrad
temp = my_adagrad(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p), full=F, regression=F)
betahats = temp[[1]]
runtimes = temp[[2]]
runtime_df$adagrad = runtimes
pred_df$adagrad = plot_prediction_error(betahats, beta, X, Y, title="Adaptive gradient descent (Adagrad)", type="prediction", regression=F, bs=rep(0, n))
est_df$adagrad = plot_prediction_error(betahats, beta, X, Y, title="Adaptive gradient descent (Adagrad)", type="estimation")
plot_betas(betahats, beta, nonzero_indexes, nonzero=T)
plot_betas(betahats, beta, nonzero_indexes, nonzero=F)
# Adam
temp = my_adam(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p), rho_1=0.9, rho_2=0.999, epsilon=1e-8, regression=F)
betahats = temp[[1]]
runtimes = temp[[2]]
runtime_df$adam = runtimes
pred_df$adam = plot_prediction_error(betahats, beta, X, Y, title="Adaptive moment estimation (Adam)", type="prediction", regression=F, bs=rep(0, n))
est_df$adam = plot_prediction_error(betahats, beta, X, Y, title="Adaptive moment estimation (Adam)", type="estimation")
plot_betas(betahats, beta, nonzero_indexes, nonzero=T)
plot_betas(betahats, beta, nonzero_indexes, nonzero=F)
# plot prediction error for ogd, adagrad, adam
ggplot(pred_df, aes(x=n)) +
geom_line(aes(y=ogd), color="red") +
geom_line(aes(y=adagrad), color="blue") +
geom_line(aes(y=adam), linetype="dashed") +
xlab("Iteration") +
ylab("") +
ggtitle("Prediction error for OGD (red), Adagrad (blue), Adam (black)")
# plot estimation error for ogd, adagrad, adam
ggplot(est_df, aes(x=n)) +
geom_line(aes(y=ogd), color="red") +
geom_line(aes(y=adagrad), color="blue") +
geom_line(aes(y=adam), linetype="dashed") +
xlab("Iteration") +
ylab("") +
ggtitle("Estimation error for OGD (red), Adagrad (blue), Adam (black)")
# plot runtime for ogd, adagrad, adam
ggplot(runtime_df, aes(x=n)) +
geom_line(aes(y=ogd), color="red") +
geom_line(aes(y=adagrad), color="blue") +
geom_line(aes(y=adam), linetype="dashed") +
xlab("Iteration") +
ylab("(s)") +
ggtitle("Runtime for OGD (red), Adagrad (blue), Adam (black)")
conv_df = data.frame("n"=1:n) # plot convergence
conv_df$err_ogd = pred_df$ogd
conv_df$err_adagrad = pred_df$adagrad
conv_df$err_adam = pred_df$adam
conv_df$time_ogd = runtime_df$ogd
conv_df$time_adagrad = runtime_df$adagrad
conv_df$time_adam = runtime_df$adam
# plot convergence for ogd, adagrad, adam
ggplot(conv_df) +
geom_line(aes(x=time_ogd, y=err_ogd), color="red") +
geom_line(aes(x=time_adagrad, y=err_adagrad), color="blue") +
geom_line(aes(x=time_adam, y=err_adam), linetype="dashed") +
xlab("Runtime (s)") +
ylab("Prediction error") +
ggtitle("Convergence for OGD (red), Adagrad (blue), Adam (black)")
runtime_df = data.frame("n"=1:n)
pred_df = data.frame("n"=1:n) # prediction error
est_df = data.frame("n"=1:n) # estimation error
# OGD
temp = my_OGD(X=X, Y=Y, lr=1e-4, beta_0=rep(0, p))
betahats = temp[[1]]
runtimes = temp[[2]]
runtime_df$ogd = runtimes
pred_df$ogd = plot_prediction_error(betahats, beta, X, Y, title="Online gradient descent (OGD)", type="prediction")
est_df$ogd = plot_prediction_error(betahats, beta, X, Y, title="Online gradient descent (OGD)", type="estimation")
plot_betas(betahats, beta, nonzero_indexes, nonzero=T) # nonzero indexes
plot_betas(betahats, beta, nonzero_indexes, nonzero=F) # zero indexes
# Adagrad
temp = my_adagrad(X=X, Y=Y, lr=1e-4, beta_0=rep(0, p), full=F)
betahats = temp[[1]]
runtimes = temp[[2]]
runtime_df$adagrad = runtimes
pred_df$adagrad = plot_prediction_error(betahats, beta, X, Y, title="Adaptive gradient descent (Adagrad)", type="prediction")
est_df$adagrad = plot_prediction_error(betahats, beta, X, Y, title="Adaptive gradient descent (Adagrad)", type="estimation")
plot_betas(betahats, beta, nonzero_indexes, nonzero=T)
plot_betas(betahats, beta, nonzero_indexes, nonzero=F)
# Adam
temp = my_adam(X=X, Y=Y, lr=1e-4, beta_0=rep(0, p), rho_1=0.9, rho_2=0.999, epsilon=1e-8)
betahats = temp[[1]]
runtimes = temp[[2]]
runtime_df$adam = runtimes
pred_df$adam = plot_prediction_error(betahats, beta, X, Y, title="Adaptive moment estimation (Adam)", type="prediction")
est_df$adam = plot_prediction_error(betahats, beta, X, Y, title="Adaptive moment estimation (Adam)", type="estimation")
plot_betas(betahats, beta, nonzero_indexes, nonzero=T)
plot_betas(betahats, beta, nonzero_indexes, nonzero=F)
# plot prediction error for ogd, adagrad, adam
ggplot(pred_df, aes(x=n)) +
geom_line(aes(y=ogd), color="red") +
geom_line(aes(y=adagrad), color="blue") +
geom_line(aes(y=adam), linetype="dashed") +
xlab("Iteration") +
ylab("") +
ggtitle("Prediction error for OGD (red), Adagrad (blue), Adam (black)")
# plot estimation error for ogd, adagrad, adam
ggplot(est_df, aes(x=n)) +
geom_line(aes(y=ogd), color="red") +
geom_line(aes(y=adagrad), color="blue") +
geom_line(aes(y=adam), linetype="dashed") +
xlab("Iteration") +
ylab("") +
ggtitle("Estimation error for OGD (red), Adagrad (blue), Adam (black)")
# plot runtime for ogd, adagrad, adam
ggplot(runtime_df, aes(x=n)) +
geom_line(aes(y=ogd), color="red") +
geom_line(aes(y=adagrad), color="blue") +
geom_line(aes(y=adam), linetype="dashed") +
xlab("Iteration") +
ylab("") +
ggtitle("Runtime for OGD (red), Adagrad (blue), Adam (black)")
conv_df = data.frame("n"=1:n) # plot convergence
conv_df$err_ogd = pred_df$ogd
conv_df$err_adagrad = pred_df$adagrad
conv_df$err_adam = pred_df$adam
conv_df$time_ogd = runtime_df$ogd
conv_df$time_adagrad = runtime_df$adagrad
conv_df$time_adam = runtime_df$adam
# plot convergence for ogd, adagrad, adam
ggplot(conv_df) +
geom_line(aes(x=time_ogd, y=err_ogd), color="red") +
geom_line(aes(x=time_adagrad, y=err_adagrad), color="blue") +
geom_line(aes(x=time_adam, y=err_adam), linetype="dashed") +
xlab("Runtime") +
ylab("Prediction error") +
ggtitle("Convergence for OGD (red), Adagrad (blue), Adam (black)")
# simulate data: classification
# X, beta same as above
probs = 1/(1+exp(-X%*%beta))
Y = rbinom(n=n, size=1, prob = probs) # Bernoulli
runtime_df = data.frame("n"=1:n)
pred_df = data.frame("n"=1:n) # prediction error
est_df = data.frame("n"=1:n) # estimation error
# OGD
temp = my_OGD(X=X, Y=Y, lr=1e-4, beta_0=rep(0, p), regression=F)
betahats = temp[[1]]
runtimes = temp[[2]]
bs = temp[[3]]
runtime_df$ogd = runtimes
pred_df$ogd = plot_prediction_error(betahats, beta, X, Y, title="Online gradient descent (OGD)", type="prediction", regression=F, bs=bs)
est_df$ogd = plot_prediction_error(betahats, beta, X, Y, title="Online gradient descent (OGD)", type="estimation")
plot_betas(betahats, beta, nonzero_indexes, nonzero=T) # nonzero indexes
plot_betas(betahats, beta, nonzero_indexes, nonzero=F) # zero indexes
# Adagrad
temp = my_adagrad(X=X, Y=Y, lr=1e-4, beta_0=rep(0, p), full=F, regression=F)
betahats = temp[[1]]
runtimes = temp[[2]]
runtime_df$adagrad = runtimes
pred_df$adagrad = plot_prediction_error(betahats, beta, X, Y, title="Adaptive gradient descent (Adagrad)", type="prediction", regression=F, bs=rep(0, n))
est_df$adagrad = plot_prediction_error(betahats, beta, X, Y, title="Adaptive gradient descent (Adagrad)", type="estimation")
plot_betas(betahats, beta, nonzero_indexes, nonzero=T)
plot_betas(betahats, beta, nonzero_indexes, nonzero=F)
# Adam
temp = my_adam(X=X, Y=Y, lr=1e-4, beta_0=rep(0, p), rho_1=0.9, rho_2=0.999, epsilon=1e-8, regression=F)
betahats = temp[[1]]
runtimes = temp[[2]]
runtime_df$adam = runtimes
pred_df$adam = plot_prediction_error(betahats, beta, X, Y, title="Adaptive moment estimation (Adam)", type="prediction", regression=F, bs=rep(0, n))
est_df$adam = plot_prediction_error(betahats, beta, X, Y, title="Adaptive moment estimation (Adam)", type="estimation")
plot_betas(betahats, beta, nonzero_indexes, nonzero=T)
plot_betas(betahats, beta, nonzero_indexes, nonzero=F)
# plot prediction error for ogd, adagrad, adam
ggplot(pred_df, aes(x=n)) +
geom_line(aes(y=ogd), color="red") +
geom_line(aes(y=adagrad), color="blue") +
geom_line(aes(y=adam), linetype="dashed") +
xlab("Iteration") +
ylab("") +
ggtitle("Prediction error for OGD (red), Adagrad (blue), Adam (black)")
# plot estimation error for ogd, adagrad, adam
ggplot(est_df, aes(x=n)) +
geom_line(aes(y=ogd), color="red") +
geom_line(aes(y=adagrad), color="blue") +
geom_line(aes(y=adam), linetype="dashed") +
xlab("Iteration") +
ylab("") +
ggtitle("Estimation error for OGD (red), Adagrad (blue), Adam (black)")
# plot runtime for ogd, adagrad, adam
ggplot(runtime_df, aes(x=n)) +
geom_line(aes(y=ogd), color="red") +
geom_line(aes(y=adagrad), color="blue") +
geom_line(aes(y=adam), linetype="dashed") +
xlab("Iteration") +
ylab("(s)") +
ggtitle("Runtime for OGD (red), Adagrad (blue), Adam (black)")
conv_df = data.frame("n"=1:n) # plot convergence
conv_df$err_ogd = pred_df$ogd
conv_df$err_adagrad = pred_df$adagrad
conv_df$err_adam = pred_df$adam
conv_df$time_ogd = runtime_df$ogd
conv_df$time_adagrad = runtime_df$adagrad
conv_df$time_adam = runtime_df$adam
# plot convergence for ogd, adagrad, adam
ggplot(conv_df) +
geom_line(aes(x=time_ogd, y=err_ogd), color="red") +
geom_line(aes(x=time_adagrad, y=err_adagrad), color="blue") +
geom_line(aes(x=time_adam, y=err_adam), linetype="dashed") +
xlab("Runtime (s)") +
ylab("Prediction error") +
ggtitle("Convergence for OGD (red), Adagrad (blue), Adam (black)")
getwd()
#'
#' Online gradient descent for linear regression.
#' @param X An n x p matrix of predictors where rows are observations and columns are predictors.
#' @param Y An n x 1 vector quantitative response variable.
#' @param lr A constant that is the learning rate.
#' @param beta_0 An p x 1 vector that is the initialization for the coefficients.
#' @param regression Boolean, regression if true, else classification.
#' @return List where first elemnt is an n x p matrix where each ith row is the coefficients for the ith iteration and the columns are predictors and second element is a nx1 vector of runtimes for each iteration.
#'
#' @export
my_OGD = function(X, Y, lr=1e-4, beta_0=rep(0, ncol(X)), regression=T) {
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
runtimes = rep(NA, n)
runtimes[1] = 0
bs = rep(NA, n) # intercept b for n iterations
if (regression) {
for (t in 1:(n-1)) {
start_time = Sys.time()
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
d_loss = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
betahats[t+1, ] = beta_t - lr*d_loss
end_time = Sys.time()
runtimes[t+1] = runtimes[t] + (end_time - start_time)
} # end for
} else {
# classification
bs[1] = 0
for (t in 1:(n-1)) {
start_time = Sys.time()
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
Y_t = Y[t]
# predict y
Z = t(beta_t)%*%x_t+bs[t]
Y_pred = 1/(1+1/exp(Z))
# calculate gradient
d_w = (1/n)*as.numeric(Y_pred-Y_t)*x_t
d_b = (1/n)*sum(Y_pred-Y_t)
# update coefficients and intercept
betahats[t+1, ] = beta_t - lr*d_w
bs[t+1] = bs[t] - lr*d_b
end_time = Sys.time()
runtimes[t+1] = runtimes[t] + (end_time - start_time)
}
} # end else
return(list(betahats, runtimes, as.matrix(bs)))
}
setwd("/Users/jwz34/Documents/Github/6520project/onlinegrad")
getwd()
devtools::document()
rm(list = c("my_adagrad", "my_adam", "my_OGD"))
devtools::document()
?my_OGD
devtools::document()
# load our package onlinegrad
install_github("joiazhang/onlinegrad")
install_github("joiazhang/6520project", subdir="onlinegrad")
?my_adagrad
# load our package onlinegrad from Github
install_github("joiazhang/6520project", subdir="onlinegrad")
library(onlinegrad)
?my_OGD
?my_adagrad
devtools::check()
?ggsave
