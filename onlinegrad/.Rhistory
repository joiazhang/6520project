betahats = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=runif(n))
rowMeans(betahats)
plot(betahats[n, ] - beta, main="Differences between last estimate and true beta")
sum(abs(betahats[n, ] - beta) > 0.5*mean(beta[nonzero_indexes])) == k
mean(betahats[n, nonzero_indexes]) - mean(betahats[n, -nonzero_indexes])
temp = matrix(rep(beta, n), nrow=n, ncol=p, byrow=T)
temp2 = betahats - temp
plot(rowMeans(temp2^2), main="Error")
plot(beta[nonzero_indexes], betahats[n, nonzero_indexes], main="K non-zero indexes for estimated and true betas")
plot(beta, main="True betas")
plot(betahats[n, ], main="Estimated betas")
plot(rowMeans(temp2^2), main="Error")
betahats = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=runif(n))
rowMeans(betahats)
plot(betahats[n, ] - beta, main="Differences between last estimate and true beta")
sum(abs(betahats[n, ] - beta) > 0.5*mean(beta[nonzero_indexes])) == k
mean(betahats[n, nonzero_indexes]) - mean(betahats[n, -nonzero_indexes])
temp = matrix(rep(beta, n), nrow=n, ncol=p, byrow=T)
temp2 = betahats - temp
plot(rowMeans(temp2^2), main="Error")
plot(beta[nonzero_indexes], betahats[n, nonzero_indexes], main="K non-zero indexes for estimated and true betas")
plot(beta, main="True betas")
plot(betahats[n, ], main="Estimated betas")
# use my_OGD on generated data
betahats = my_OGD(type="regression", X=X, Y=Y, lr=0.00001, beta_0=rep(0, p), N=100)
# use my_OGD on generated data
betahats = my_OGD(type="regression", X=X, Y=Y, lr=0.00001, beta_0=rep(0, p), N=100)
# function for online gradient descent (OGD)
# type: "classification" or "regresssion"
# X: rows are observations, columns are predictors
# Y: response variable
# learning rate (constant)
# beta_0: initialization for the estimate
# N: number of iterations for each data point (each row of X, Y)
# returns betahats for each data point and its N interations as a 3D array
my_OGD = function(type, X, Y, lr, beta_0, N) {
if (type!='classification'&&type!='regression') {
stop("Argument 'type' must be 'classification' or 'regression'")
}
n = nrow(X)
betahats = array(data=rep(NA, n*p*N), dim = c(N, p, n)) # n arrays that are each Nxp arrays
betahats[1, , ] = beta_0 # initialize
if (type=='classification') {
} else {
# type is regression
for (k in 1:n) { # for each data point (each row of X, Y)
for (i in 1:(N-1)) { # for each iteration
d_loss = 2*as.matrix(betahats[i, , k])%*%as.matrix(t(X[k, ]))%*%as.matrix(X[k, ])-2*as.matrix(X[k, ])%*%as.matrix(Y[k]) # OLS loss
betahats[i+1, , k] = betahats[i, , k] - lr*d_loss
} # end for i
} # end for k
return(betahats)
} # end else for regression
}
# use my_OGD on generated data
betahats = my_OGD(type="regression", X=X, Y=Y, lr=0.00001, beta_0=rep(0, p), N=100)
rowMeans(betahats)
plot(betahats[n, ] - beta, main="Differences between last estimate and true beta")
# use my_OGD on generated data
betahats = my_OGD(type="regression", X=X, Y=Y, lr=0.00001, beta_0=rep(0, p), N=100)
plot(betahats[n, ] - beta, main="Differences between last estimate and true beta")
betahats
str(betahats)
# use my_OGD on generated data
betahats = my_OGD(type="regression", X=X, Y=Y, lr=0.00001, beta_0=rep(0, p), N=100)
# Online gradient descent for regression
my_OGD = function(type, X, Y, lr, beta_0) {
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
for (t in 1:(n-1)) {
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
betahats[t+1, ] = beta_t - lr*as.numeric(y_t_hat-Y_t)*x_t
}
return(betahats)
}
betahats = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=runif(n))
plot(betahats[n, ] - beta, main="Differences between last estimate and true beta")
sum(abs(betahats[n, ] - beta) > 0.5*mean(beta[nonzero_indexes])) == k
mean(betahats[n, nonzero_indexes]) - mean(betahats[n, -nonzero_indexes])
temp = matrix(rep(beta, n), nrow=n, ncol=p, byrow=T)
temp2 = betahats - temp
plot(rowMeans(temp2^2), main="Error")
plot(beta[nonzero_indexes], betahats[n, nonzero_indexes], main="K non-zero indexes for estimated and true betas")
plot(beta, main="True betas")
plot(betahats[n, ], main="Estimated betas")
set.seed(6520)
# simulate data: regression
n = 100 # sample size
p = 200 # number of predictors
# beta
k = round(0.05*p, 0) # number of nonzero coefficients
sd_beta = 0.01
nonzero_indexes = sample.int(n=p, size=k)
beta = rep(0, p)
beta[nonzero_indexes] = rnorm(n=k, mean=100, sd=sd_beta)
sum(which(beta !=0) != sort(nonzero_indexes)) # test that we made the right indexes nonzero
beta = as.matrix(beta)
# x
X = matrix(rnorm(n=n*p, mean=0, sd=5), nrow=n)
# epsilon
E = matrix(rnorm(n=n, mean=0, sd=1), nrow=n)
# y
Y = X%*%beta + E
# note that in the online setting, each t^th row of X and Y is for time t
# simulate data: classification
# X, beta same as above
probs = 1/(1+exp(-X%*%beta))
Y = rbinom(n=n, size=1, prob = probs) # Bernoulli
# function for online gradient descent (OGD)
# type: "classification" or "regresssion"
# X: rows are observations, columns are predictors
# Y: response variable
# learning rate (constant)
# beta_0: initialization for the estimate
# N: number of iterations for each data point (each row of X, Y)
# returns betahats for each data point and its N interations as a 3D array
my_OGD = function(type, X, Y, lr, beta_0, N) {
if (type!='classification'&&type!='regression') {
stop("Argument 'type' must be 'classification' or 'regression'")
}
n = nrow(X)
betahats = array(data=rep(NA, n*p*N), dim = c(N, p, n)) # n arrays that are each Nxp arrays
betahats[1, , ] = beta_0 # initialize
if (type=='classification') {
} else {
# type is regression
for (k in 1:n) { # for each data point (each row of X, Y)
for (i in 1:(N-1)) { # for each iteration
d_loss = 2*as.matrix(betahats[i, , k])%*%as.matrix(t(X[k, ]))%*%as.matrix(X[k, ])-2*as.matrix(X[k, ])%*%as.matrix(Y[k]) # OLS loss
betahats[i+1, , k] = betahats[i, , k] - lr*d_loss
} # end for i
} # end for k
return(betahats)
} # end else for regression
}
# use my_OGD on generated data
betahats = my_OGD(type="regression", X=X, Y=Y, lr=0.00001, beta_0=rep(0, p), N=100)
# Online gradient descent for regression
my_OGD = function(type, X, Y, lr, beta_0) {
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
for (t in 1:(n-1)) {
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
betahats[t+1, ] = beta_t - lr*as.numeric(y_t_hat-Y_t)*x_t
}
return(betahats)
}
betahats = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=runif(n))
plot(betahats[n, ] - beta, main="Differences between last estimate and true beta")
sum(abs(betahats[n, ] - beta) > 0.5*mean(beta[nonzero_indexes])) == k
mean(betahats[n, nonzero_indexes]) - mean(betahats[n, -nonzero_indexes])
temp = matrix(rep(beta, n), nrow=n, ncol=p, byrow=T)
temp2 = betahats - temp
plot(rowMeans(temp2^2), main="Error")
plot(beta[nonzero_indexes], betahats[n, nonzero_indexes], main="K non-zero indexes for estimated and true betas")
plot(beta, main="True betas")
plot(betahats[n, ], main="Estimated betas")
set.seed(6520)
# simulate data: regression
n = 100 # sample size
p = 200 # number of predictors
# beta
k = round(0.05*p, 0) # number of nonzero coefficients
sd_beta = 0.01
nonzero_indexes = sample.int(n=p, size=k)
beta = rep(0, p)
beta[nonzero_indexes] = rnorm(n=k, mean=100, sd=sd_beta)
sum(which(beta !=0) != sort(nonzero_indexes)) # test that we made the right indexes nonzero
beta = as.matrix(beta)
# x
X = matrix(rnorm(n=n*p, mean=0, sd=5), nrow=n)
# epsilon
E = matrix(rnorm(n=n, mean=0, sd=1), nrow=n)
# y
Y = X%*%beta + E
# note that in the online setting, each t^th row of X and Y is for time t
# Online gradient descent for regression
my_OGD = function(type, X, Y, lr, beta_0) {
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
for (t in 1:(n-1)) {
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
betahats[t+1, ] = beta_t - lr*as.numeric(y_t_hat-Y_t)*x_t
}
return(betahats)
}
betahats = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=runif(n))
plot(betahats[n, ] - beta, main="Differences between last estimate and true beta")
sum(abs(betahats[n, ] - beta) > 0.5*mean(beta[nonzero_indexes])) == k
mean(betahats[n, nonzero_indexes]) - mean(betahats[n, -nonzero_indexes])
temp = matrix(rep(beta, n), nrow=n, ncol=p, byrow=T)
temp2 = betahats - temp
plot(rowMeans(temp2^2), main="Error")
plot(beta[nonzero_indexes], betahats[n, nonzero_indexes], main="K non-zero indexes for estimated and true betas")
plot(beta, main="True betas")
plot(betahats[n, ], main="Estimated betas")
# function for online gradient descent (OGD)
# type: "classification" or "regresssion"
# X: rows are observations, columns are predictors
# Y: response variable
# learning rate (constant)
# beta_0: initialization for the estimate
# N: number of iterations for each data point (each row of X, Y)
# returns betahats for each data point and its N interations as a 3D array
my_OGD = function(type, X, Y, lr, beta_0, N) {
if (type!='classification'&&type!='regression') {
stop("Argument 'type' must be 'classification' or 'regression'")
}
n = nrow(X)
betahats = array(data=rep(NA, n*p*N), dim = c(N, p, n)) # n arrays that are each Nxp arrays
betahats[1, , ] = beta_0 # initialize
if (type=='classification') {
} else {
# type is regression
for (k in 1:n) { # for each data point (each row of X, Y)
for (i in 1:1) { # for each iteration
d_loss = 2*as.matrix(betahats[i, , k])%*%as.matrix(t(X[k, ]))%*%as.matrix(X[k, ])-2*as.matrix(X[k, ])%*%as.matrix(Y[k]) # OLS loss
betahats[i+1, , k] = betahats[i, , k] - lr*d_loss
} # end for i
} # end for k
return(betahats)
} # end else for regression
}
rm(list=ls())
set.seed(6520)
# simulate data: regression
n = 100 # sample size
p = 200 # number of predictors
# beta
k = round(0.05*p, 0) # number of nonzero coefficients
sd_beta = 0.01
nonzero_indexes = sample.int(n=p, size=k)
beta = rep(0, p)
beta[nonzero_indexes] = rnorm(n=k, mean=100, sd=sd_beta)
sum(which(beta !=0) != sort(nonzero_indexes)) # test that we made the right indexes nonzero
beta = as.matrix(beta)
# x
X = matrix(rnorm(n=n*p, mean=0, sd=5), nrow=n)
# epsilon
E = matrix(rnorm(n=n, mean=0, sd=1), nrow=n)
# y
Y = X%*%beta + E
# note that in the online setting, each t^th row of X and Y is for time t
# function for online gradient descent (OGD)
# type: "classification" or "regresssion"
# X: rows are observations, columns are predictors
# Y: response variable
# learning rate (constant)
# beta_0: initialization for the estimate
# N: number of iterations for each data point (each row of X, Y)
# returns betahats for each data point and its N interations as a 3D array
my_OGD = function(type, X, Y, lr, beta_0, N) {
if (type!='classification'&&type!='regression') {
stop("Argument 'type' must be 'classification' or 'regression'")
}
n = nrow(X)
betahats = array(data=rep(NA, n*p*N), dim = c(N, p, n)) # n arrays that are each Nxp arrays
betahats[1, , ] = beta_0 # initialize
if (type=='classification') {
} else {
# type is regression
for (k in 1:n) { # for each data point (each row of X, Y)
for (i in 1:1) { # for each iteration
d_loss = 2*as.matrix(betahats[i, , k])%*%as.matrix(t(X[k, ]))%*%as.matrix(X[k, ])-2*as.matrix(X[k, ])%*%as.matrix(Y[k]) # OLS loss
betahats[i+1, , k] = betahats[i, , k] - lr*d_loss
} # end for i
} # end for k
return(betahats)
} # end else for regression
}
# use my_OGD on generated data
betahats = my_OGD(type="regression", X=X, Y=Y, lr=0.00001, beta_0=rep(0, p), N=100)
betahats
dim(betahats)
betahats[1]
betahats[[1]]
str(betahats)
betahats[1, ]
betahats[1, 1, ]
betahats[2, 1, ]
betahats[1, , 1]
betahats[, , 1]
betahats[n, , 1]
betahats[2, , 1]
betahats[3, , 1]
betahats[, 1, ]
betahats
betahats[, , ,]
betahats[1, , ,]
betahats[1, , ]
k
betahats[1, , k]
betahats[1, 2, k]
betahats[, 2, k]
betahats[, , k]
betahats[1, , ]
betahats[1, , ]
dim(betahats[1, , ])
dim(betahats[1, , ])
betahats[1, , ]
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
beta_0=runif(n)
betahats[1, ] = beta_0
x_t = as.matrix(X[t, ])
t = 1
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
# betahats[t+1, ] = beta_t - lr*as.numeric(y_t_hat-Y_t)*x_t
d_loss = 2*as.matrix(betahats[i, , k])%*%as.matrix(t(X[k, ]))%*%as.matrix(X[k, ])-2*as.matrix(X[k, ])%*%as.matrix(Y[k]) # OLS loss
d_loss = 2*t(beta_t)%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
2*t(beta_t)%*%t(x_t)%*%x_t
dim(t(beta_t))
dim(t(x_t))
2*beta_t%*%t(x_t)%*%x_t
dim(2*beta_t%*%t(x_t)%*%x_t)
dim(2*x_t%*%Y_t)
2*x_t%*%Y_t
d_loss = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
betahats[t+1, ] = beta_t - lr*d_loss
lr = 0.00001
betahats[t+1, ] = beta_t - lr*d_loss
betahats[t+1, ]
summary(betahats[t+1, ])
betahats[t+1, nonzero_indexes]
betahats[t+1, -nonzero_indexes]
mean(betahats[t+1, nonzero_indexes])
mean(betahats[t+1, -nonzero_indexes])
# Online gradient descent for regression
my_OGD = function(type, X, Y, lr, beta_0) {
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
for (t in 1:(n-1)) {
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
# betahats[t+1, ] = beta_t - lr*as.numeric(y_t_hat-Y_t)*x_t
# d_loss = 2*as.matrix(betahats[i, , k])%*%as.matrix(t(X[k, ]))%*%as.matrix(X[k, ])-2*as.matrix(X[k, ])%*%as.matrix(Y[k]) # OLS loss
# betahats[i+1, , k] = betahats[i, , k] - lr*d_loss
d_loss = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
betahats[t+1, ] = beta_t - lr*d_loss
}
return(betahats)
}
# Online gradient descent for regression
my_OGD = function(type, X, Y, lr, beta_0) {
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
for (t in 1:(n-1)) {
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
# betahats[t+1, ] = beta_t - lr*as.numeric(y_t_hat-Y_t)*x_t
# d_loss = 2*as.matrix(betahats[i, , k])%*%as.matrix(t(X[k, ]))%*%as.matrix(X[k, ])-2*as.matrix(X[k, ])%*%as.matrix(Y[k]) # OLS loss
# betahats[i+1, , k] = betahats[i, , k] - lr*d_loss
d_loss = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
betahats[t+1, ] = beta_t - lr*d_loss
}
return(betahats)
}
# Online gradient descent for regression
my_OGD = function(type, X, Y, lr, beta_0) {
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
for (t in 1:(n-1)) {
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
# betahats[t+1, ] = beta_t - lr*as.numeric(y_t_hat-Y_t)*x_t # least means squares
d_loss = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
betahats[t+1, ] = beta_t - lr*d_loss
}
return(betahats)
}
betahats = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=runif(n))
plot(betahats[n, ] - beta, main="Differences between last estimate and true beta")
sum(abs(betahats[n, ] - beta) > 0.5*mean(beta[nonzero_indexes])) == k
mean(betahats[n, nonzero_indexes]) - mean(betahats[n, -nonzero_indexes])
temp = matrix(rep(beta, n), nrow=n, ncol=p, byrow=T)
temp2 = betahats - temp
plot(rowMeans(temp2^2), main="Error")
plot(beta[nonzero_indexes], betahats[n, nonzero_indexes], main="K non-zero indexes for estimated and true betas")
plot(beta, main="True betas")
plot(betahats[n, ], main="Estimated betas")
beta_0 = runif(n)
beta_0 = rep(0, p)
beta_0 = runif(n)
beta_0 = rep(0, p)
betahats = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=beta_0)
plot(betahats[n, ] - beta, main="Differences between last estimate and true beta")
sum(abs(betahats[n, ] - beta) > 0.5*mean(beta[nonzero_indexes])) == k
mean(betahats[n, nonzero_indexes]) - mean(betahats[n, -nonzero_indexes])
temp = matrix(rep(beta, n), nrow=n, ncol=p, byrow=T)
temp2 = betahats - temp
plot(rowMeans(temp2^2), main="Error")
plot(beta[nonzero_indexes], betahats[n, nonzero_indexes], main="K non-zero indexes for estimated and true betas")
plot(beta, main="True betas")
plot(betahats[n, ], main="Estimated betas")
dim(betahats)
betahats[n, nonzero_indexes]
mean(betahats[n, nonzero_indexes])
mean(betahats[n, -nonzero_indexes])
mean(beta[nonzero_indexes])
library(devtools)
library("devtools")
library("roxygen2")
library(testthat)
install.packages(c("devtools", "roxygen2", "testthat", "knitr", "covr"))
install.packages(c("devtools", "roxygen2", "testthat", "knitr", "covr"))
install.packages(c("devtools", "roxygen2", "testthat", "knitr", "covr"))
install.packages(c("devtools", "roxygen2", "testthat", "knitr", "covr"))
install.packages(c("devtools", "roxygen2", "testthat", "knitr", "covr"))
install.packages(c("devtools", "roxygen2", "testthat", "knitr", "covr"))
install.packages(c("devtools", "roxygen2", "testthat", "knitr", "covr"))
install.packages("devtools")
install.packages("roxygen2")
library("testthat")
library("knitr")
library(covr)
install.packages("covr")
ls
getwd()
setwd("/Users/jwz34/Documents/Github/6520project")
getwd()
devtools::create("6520Package")
devtools::create("onlinegrad")
dim(betahats)
# Online gradient descent for regression
my_OGD = function(X, Y, lr, beta_0) {
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
for (t in 1:(n-1)) {
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
# betahats[t+1, ] = beta_t - lr*as.numeric(y_t_hat-Y_t)*x_t # least means squares
d_loss = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
betahats[t+1, ] = beta_t - lr*d_loss
}
return(betahats)
}
beta_0 = runif(p)
beta_0 = rep(0, p)
betahats = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=beta_0)
plot(betahats[n, ] - beta, main="Differences between last estimate and true beta")
sum(abs(betahats[n, ] - beta) > 0.5*mean(beta[nonzero_indexes])) == k
mean(betahats[n, nonzero_indexes]) - mean(betahats[n, -nonzero_indexes])
temp = matrix(rep(beta, n), nrow=n, ncol=p, byrow=T)
temp2 = betahats - temp
plot(rowMeans(temp2^2), main="Error")
plot(beta[nonzero_indexes], betahats[n, nonzero_indexes], main="K non-zero indexes for estimated and true betas")
plot(beta, main="True betas")
plot(betahats[n, ], main="Estimated betas")
beta_0 = runif(p)
# beta_0 = rep(0, p)
betahats = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=beta_0)
plot(betahats[n, ] - beta, main="Differences between last estimate and true beta")
sum(abs(betahats[n, ] - beta) > 0.5*mean(beta[nonzero_indexes])) == k
mean(betahats[n, nonzero_indexes]) - mean(betahats[n, -nonzero_indexes])
temp = matrix(rep(beta, n), nrow=n, ncol=p, byrow=T)
temp2 = betahats - temp
plot(rowMeans(temp2^2), main="Error")
plot(beta[nonzero_indexes], betahats[n, nonzero_indexes], main="K non-zero indexes for estimated and true betas")
plot(beta, main="True betas")
plot(betahats[n, ], main="Estimated betas")
devtools::document()
getwd()
devtools::document()
rlang::last_trace()
getwd()
setwd("/Users/jwz34/Documents/Github/6520project/onlinegrad")
getwd()
rlang::last_trace(drop = FALSE)
getwd()
devtools::document()
devtools::install()
library(onlinegrad)
?my_OGD
plot()
?rnorm
?my_OGD
