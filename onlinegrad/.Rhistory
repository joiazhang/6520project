G_t = G_t + g_t%*%t(g_t)
diag_G_t = diag(diag(G_t), nrow=p, ncol=p)
betahats[t+1, ] = beta_t - lr*as.matrix(solve(as.matrix(expm::sqrtm(G_t))))%*%g_t
dim(beta_t)
dim(g_t)
dim(expm::sqrtm(G_t))
dim(G_t)
sqrtm(G_t)
count.na(G_t)
sum(G_t==NA)
G_t
dim(G_t)
G_t[1, 1]
t = 1
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
g_vec = matrix(nrow=n, ncol=p) # save matrix for the gradients where each gradient g_t is the t^{th} row of the matrix
G_t = matrix(data=rep(0, p^2), nrow=p, ncol=p) # matrix that is a cumulative sum
runtimes = rep(NA, n)
runtimes[1] = 0
start_time = Sys.time()
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
g_vec[t, ] = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
g_t = as.matrix(g_vec[t, ])
G_t = G_t + g_t%*%t(g_t)
diag_G_t = diag(diag(G_t), nrow=p, ncol=p)
betahats[t+1, ] = beta_t - lr*as.matrix(solve(as.matrix(expm::sqrtm(G_t))))%*%g_t
t = 2
start_time = Sys.time()
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
g_vec[t, ] = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
g_t = as.matrix(g_vec[t, ])
G_t = G_t + g_t%*%t(g_t)
diag_G_t = diag(diag(G_t), nrow=p, ncol=p)
betahats[t+1, ] = beta_t - lr*as.matrix(solve(as.matrix(expm::sqrtm(G_t))))%*%g_t
betahats[1, ]
betahats[2, ]
betahats[3, ]
betahats[1, ]
beta_0
t = 1
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
g_vec = matrix(nrow=n, ncol=p) # save matrix for the gradients where each gradient g_t is the t^{th} row of the matrix
G_t = matrix(data=rep(0, p^2), nrow=p, ncol=p) # matrix that is a cumulative sum
runtimes = rep(NA, n)
runtimes[1] = 0
betahats[1, ]
start_time = Sys.time()
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
g_vec[t, ] = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
g_t = as.matrix(g_vec[t, ])
G_t = G_t + g_t%*%t(g_t)
diag_G_t = diag(diag(G_t), nrow=p, ncol=p)
betahats[t+1, ] = beta_t - lr*as.matrix(solve(as.matrix(expm::sqrtm(G_t))))%*%g_t
betahats[t+1, ]
t+1
bethats[1, ]
betahats[1, ]
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
runtimes = rep(NA, n)
runtimes[1] = 0
# classification
bs = rep(NA, n) # intercept b for n iterations
x_t = as.matrix(X[t, ])
dim(x_t)
beta_t = as.matrix(betahats[t, ])
dim(beta_t)
dim(t(beta_t))
dim(t(beta_t)%*%x_t)
?exp
exp(0)
exp(1)
Z = t(beta_t)%*%x_t+bs[t]
Z
bs[t]
t
# classification
bs = rep(NA, n) # intercept b for n iterations
bs[1] = 0
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
Z = t(beta_t)%*%x_t+bs[t]
Z
y_pred = 1/(1+1/exp(Z))
y_pred
dim(x_t)
dim(Y)
length(Y)
Y_t
Y
Y_t = Y[t]
# predict y
Z = t(beta_t)%*%x_t+bs[t]
Y_pred = 1/(1+1/exp(Z))
Y_pred-Y_t
Y_pred
Y_t
# calculate gradient
d_w = (1/n)*(Y_pred-Y_t)*x_t
(Y_pred-Y_t)
dim((Y_pred-Y_t))
x_t
# calculate gradient
d_w = (1/n)*as.numeric(Y_pred-Y_t)*x_t
d_w
dim(x_t)
as.numeric(Y_pred-Y_t)
as.numeric(Y_pred-Y_t)*x_t
x_t
# calculate gradient
d_w = (1/n)*as.numeric(Y_pred-Y_t)*x_t # TODO: dimensions are not correct, should be px1
dim(d_w)
# predict y
Z = t(beta_t)%*%x_t+bs[t]
Y_pred = 1/(1+1/exp(Z))
# calculate gradient
d_w = (1/n)*as.numeric(Y_pred-Y_t)*x_t
d_b = (1/n)*sum(y_pred-y)
# calculate gradient
d_w = (1/n)*as.numeric(Y_pred-Y_t)*x_t
dim(d_w)
d_b = (1/n)*sum(y_pred-Y_t)
d_b
source("~/Documents/Github/6520project/onlinegrad/R/my_OGD.R", echo=TRUE)
#' @param Y An n x 1 vector quantitative response variable.
#' @param lr A constant that is the learning rate.
#' @param beta_0 An p x 1 vector that is the initialization for the coefficients.
#' @param regression Boolean, regression if true, else classification
#' @return List where first elemnt is an n x p matrix where each ith row is the coefficients for the ith iteration and the columns are predictors and second element is a nx1 vector of runtimes for each iteration.
#' @examples
#' my_OGD(X=X, Y=Y, lr=0.00001, beta_0=rep(0, ncol(X)))
#' my_OGD(X=X, Y=Y, lr=0.00001, beta_0=runif(ncol(X)))
#'
#' @export
my_OGD = function(X, Y, lr, beta_0, regression=T) {
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
runtimes = rep(NA, n)
runtimes[1] = 0
if (regression) {
for (t in 1:(n-1)) {
start_time = Sys.time()
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
d_loss = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
betahats[t+1, ] = beta_t - lr*d_loss
end_time = Sys.time()
runtimes[t+1] = runtimes[t] + (end_time - start_time)
} # end for
} else {
# classification
bs = rep(NA, n) # intercept b for n iterations
bs[1] = 0
for (t in 1:(n-1)) {
start_time = Sys.time()
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
Y_t = Y[t]
# predict y
Z = t(beta_t)%*%x_t+bs[t]
Y_pred = 1/(1+1/exp(Z))
# calculate gradient
d_w = (1/n)*as.numeric(Y_pred-Y_t)*x_t
d_b = (1/n)*sum(y_pred-Y_t)
# update coefficients and intercept
betahats[t+1, ] = beta_t - lr*d_loss
b = b - lr*d_b
end_time = Sys.time()
runtimes[t+1] = runtimes[t] + (end_time - start_time)
}
} # end else
return(list(betahats, runtimes))
}
#' @param Y An n x 1 vector quantitative response variable.
#' @param lr A constant that is the learning rate.
#' @param beta_0 An p x 1 vector that is the initialization for the coefficients.
#' @param regression Boolean, regression if true, else classification
#' @return List where first elemnt is an n x p matrix where each ith row is the coefficients for the ith iteration and the columns are predictors and second element is a nx1 vector of runtimes for each iteration.
#' @examples
#' my_OGD(X=X, Y=Y, lr=0.00001, beta_0=rep(0, ncol(X)))
#' my_OGD(X=X, Y=Y, lr=0.00001, beta_0=runif(ncol(X)))
#'
#' @export
my_OGD = function(X, Y, lr, beta_0, regression=T) {
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
runtimes = rep(NA, n)
runtimes[1] = 0
if (regression) {
for (t in 1:(n-1)) {
start_time = Sys.time()
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
d_loss = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
betahats[t+1, ] = beta_t - lr*d_loss
end_time = Sys.time()
runtimes[t+1] = runtimes[t] + (end_time - start_time)
} # end for
} else {
# classification
bs = rep(NA, n) # intercept b for n iterations
bs[1] = 0
for (t in 1:(n-1)) {
start_time = Sys.time()
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
Y_t = Y[t]
# predict y
Z = t(beta_t)%*%x_t+bs[t]
Y_pred = 1/(1+1/exp(Z))
# calculate gradient
d_w = (1/n)*as.numeric(Y_pred-Y_t)*x_t
d_b = (1/n)*sum(y_pred-Y_t)
# update coefficients and intercept
betahats[t+1, ] = beta_t - lr*d_loss
b = b - lr*d_b
end_time = Sys.time()
runtimes[t+1] = runtimes[t] + (end_time - start_time)
}
} # end else
return(list(betahats, runtimes))
}
#' @param Y An n x 1 vector quantitative response variable.
#' @param lr A constant that is the learning rate.
#' @param beta_0 An p x 1 vector that is the initialization for the coefficients.
#' @param regression Boolean, regression if true, else classification
#' @return List where first elemnt is an n x p matrix where each ith row is the coefficients for the ith iteration and the columns are predictors and second element is a nx1 vector of runtimes for each iteration.
#' @examples
#' my_OGD(X=X, Y=Y, lr=0.00001, beta_0=rep(0, ncol(X)))
#' my_OGD(X=X, Y=Y, lr=0.00001, beta_0=runif(ncol(X)))
#'
#' @export
my_OGD = function(X, Y, lr, beta_0, regression=T) {
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
runtimes = rep(NA, n)
runtimes[1] = 0
if (regression) {
for (t in 1:(n-1)) {
start_time = Sys.time()
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
d_loss = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
betahats[t+1, ] = beta_t - lr*d_loss
end_time = Sys.time()
runtimes[t+1] = runtimes[t] + (end_time - start_time)
} # end for
} else {
# classification
bs = rep(NA, n) # intercept b for n iterations
bs[1] = 0
for (t in 1:(n-1)) {
start_time = Sys.time()
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
Y_t = Y[t]
# predict y
Z = t(beta_t)%*%x_t+bs[t]
Y_pred = 1/(1+1/exp(Z))
# calculate gradient
d_w = (1/n)*as.numeric(Y_pred-Y_t)*x_t
d_b = (1/n)*sum(y_pred-Y_t)
# update coefficients and intercept
betahats[t+1, ] = beta_t - lr*d_loss
b = b - lr*d_b
end_time = Sys.time()
runtimes[t+1] = runtimes[t] + (end_time - start_time)
}
} # end else
return(list(betahats, runtimes))
}
# OGD classification
temp = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p), regression=F)
# OGD classification
temp = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p), regression=F)
#' @param Y An n x 1 vector quantitative response variable.
#' @param lr A constant that is the learning rate.
#' @param beta_0 An p x 1 vector that is the initialization for the coefficients.
#' @param regression Boolean, regression if true, else classification
#' @return List where first elemnt is an n x p matrix where each ith row is the coefficients for the ith iteration and the columns are predictors and second element is a nx1 vector of runtimes for each iteration.
#' @examples
#' my_OGD(X=X, Y=Y, lr=0.00001, beta_0=rep(0, ncol(X)))
#' my_OGD(X=X, Y=Y, lr=0.00001, beta_0=runif(ncol(X)))
#'
#' @export
my_OGD = function(X, Y, lr, beta_0, regression=T) {
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
runtimes = rep(NA, n)
runtimes[1] = 0
if (regression) {
for (t in 1:(n-1)) {
start_time = Sys.time()
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
d_loss = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
betahats[t+1, ] = beta_t - lr*d_loss
end_time = Sys.time()
runtimes[t+1] = runtimes[t] + (end_time - start_time)
} # end for
} else {
# classification
bs = rep(NA, n) # intercept b for n iterations
bs[1] = 0
for (t in 1:(n-1)) {
start_time = Sys.time()
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
Y_t = Y[t]
# predict y
Z = t(beta_t)%*%x_t+bs[t]
Y_pred = 1/(1+1/exp(Z))
# calculate gradient
d_w = (1/n)*as.numeric(Y_pred-Y_t)*x_t
d_b = (1/n)*sum(y_pred-Y_t)
# update coefficients and intercept
betahats[t+1, ] = beta_t - lr*d_loss
b = b - lr*d_b
end_time = Sys.time()
runtimes[t+1] = runtimes[t] + (end_time - start_time)
}
} # end else
return(list(betahats, runtimes))
}
# OGD classification
temp = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p), regression=F)
#' @param Y An n x 1 vector quantitative response variable.
#' @param lr A constant that is the learning rate.
#' @param beta_0 An p x 1 vector that is the initialization for the coefficients.
#' @param regression Boolean, regression if true, else classification
#' @return List where first elemnt is an n x p matrix where each ith row is the coefficients for the ith iteration and the columns are predictors and second element is a nx1 vector of runtimes for each iteration.
#' @examples
#' my_OGD(X=X, Y=Y, lr=0.00001, beta_0=rep(0, ncol(X)))
#' my_OGD(X=X, Y=Y, lr=0.00001, beta_0=runif(ncol(X)))
#'
#' @export
my_OGD = function(X, Y, lr, beta_0, regression=T) {
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
runtimes = rep(NA, n)
runtimes[1] = 0
if (regression) {
for (t in 1:(n-1)) {
start_time = Sys.time()
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
d_loss = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
betahats[t+1, ] = beta_t - lr*d_loss
end_time = Sys.time()
runtimes[t+1] = runtimes[t] + (end_time - start_time)
} # end for
} else {
# classification
bs = rep(NA, n) # intercept b for n iterations
bs[1] = 0
for (t in 1:(n-1)) {
start_time = Sys.time()
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
Y_t = Y[t]
# predict y
Z = t(beta_t)%*%x_t+bs[t]
Y_pred = 1/(1+1/exp(Z))
# calculate gradient
d_w = (1/n)*as.numeric(Y_pred-Y_t)*x_t
d_b = (1/n)*sum(y_pred-Y_t)
# update coefficients and intercept
betahats[t+1, ] = beta_t - lr*d_loss
b = b - lr*d_b
end_time = Sys.time()
runtimes[t+1] = runtimes[t] + (end_time - start_time)
}
} # end else
return(list(betahats, runtimes))
}
rm(list=ls())
?my_ogd
library(onlinegrad)
?my_ogd
rm(list=ls())
set.seed(6520)
library(ggplot2)
library(expm) # for sqrtm
# setwd("/Users/jwz34/Documents/Github/6520project/onlinegrad")
# devtools::install()
library(onlinegrad)
getwd()
# simulate data: regression
n = 100 # sample size
p = 200 # number of predictors
# beta
k = round(0.05*p, 0) # number of nonzero coefficients
sd_beta = 0.01
nonzero_indexes = sample.int(n=p, size=k)
beta = rep(0, p)
beta[nonzero_indexes] = rnorm(n=k, mean=100, sd=sd_beta)
sum(which(beta !=0) != sort(nonzero_indexes)) # test that we made the right indexes nonzero
beta = as.matrix(beta)
# x
X = matrix(rnorm(n=n*p, mean=0, sd=5), nrow=n)
# epsilon
E = matrix(rnorm(n=n, mean=0, sd=1), nrow=n)
Y = X%*%beta + E
# note that in the online setting, each t^th row of X and Y is for time t
```
# simulate data: regression
n = 100 # sample size
p = 200 # number of predictors
# beta
k = round(0.05*p, 0) # number of nonzero coefficients
sd_beta = 0.01
nonzero_indexes = sample.int(n=p, size=k)
beta = rep(0, p)
beta[nonzero_indexes] = rnorm(n=k, mean=100, sd=sd_beta)
sum(which(beta !=0) != sort(nonzero_indexes)) # test that we made the right indexes nonzero
beta = as.matrix(beta)
# x
X = matrix(rnorm(n=n*p, mean=0, sd=5), nrow=n)
# epsilon
E = matrix(rnorm(n=n, mean=0, sd=1), nrow=n)
# y
Y = X%*%beta + E
# note that in the online setting, each t^th row of X and Y is for time t
# simulate data: regression
n = 100 # sample size
p = 200 # number of predictors
# beta
k = round(0.05*p, 0) # number of nonzero coefficients
sd_beta = 0.01
nonzero_indexes = sample.int(n=p, size=k)
beta = rep(0, p)
beta[nonzero_indexes] = rnorm(n=k, mean=100, sd=sd_beta)
sum(which(beta !=0) != sort(nonzero_indexes)) # test that we made the right indexes nonzero
beta = as.matrix(beta)
# x
X = matrix(rnorm(n=n*p, mean=0, sd=5), nrow=n)
# epsilon
E = matrix(rnorm(n=n, mean=0, sd=1), nrow=n)
# y
Y = X%*%beta + E
# note that in the online setting, each t^th row of X and Y is for time t
# simulate data: regression
n = 100 # sample size
p = 200 # number of predictors
# beta
k = round(0.05*p, 0) # number of nonzero coefficients
sd_beta = 0.01
nonzero_indexes = sample.int(n=p, size=k)
beta = rep(0, p)
beta[nonzero_indexes] = rnorm(n=k, mean=100, sd=sd_beta)
sum(which(beta !=0) != sort(nonzero_indexes)) # test that we made the right indexes nonzero
beta = as.matrix(beta)
# x
X = matrix(rnorm(n=n*p, mean=0, sd=5), nrow=n)
# epsilon
E = matrix(rnorm(n=n, mean=0, sd=1), nrow=n)
Y = X%*%beta + E
# simulate data: regression
n = 100 # sample size
p = 200 # number of predictors
# beta
k = round(0.05*p, 0) # number of nonzero coefficients
sd_beta = 0.01
nonzero_indexes = sample.int(n=p, size=k)
beta = rep(0, p)
beta[nonzero_indexes] = rnorm(n=k, mean=100, sd=sd_beta)
sum(which(beta !=0) != sort(nonzero_indexes)) # test that we made the right indexes nonzero
beta = as.matrix(beta)
# x
X = matrix(rnorm(n=n*p, mean=0, sd=5), nrow=n)
# epsilon
E = matrix(rnorm(n=n, mean=0, sd=1), nrow=n)
# y
Y = X%*%beta + E
# note that in the online setting, each t^th row of X and Y is for time t
