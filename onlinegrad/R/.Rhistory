H = list() # list of nxn matrices
H[1] = matrix(nrow=5, ncol=5)
H[[]] = matrix(nrow=5, ncol=5)
H[[1]] = matrix(nrow=5, ncol=5)
H
H[[2]] = matrix(data=rep(1, 25), nrow=5, ncol=5)
H
diag(nrow=mn)
diag(nrow=5)
type(diag(nrow=5))
str(diag(nrow=5))
epsilon = 0.0001
n = 5
epsilon*diag(nrow=n)
# simulate data: regression
n = 100 # sample size
p = 200 # number of predictors
# beta
k = round(0.05*p, 0) # number of nonzero coefficients
sd_beta = 0.01
nonzero_indexes = sample.int(n=p, size=k)
beta = rep(0, p)
beta[nonzero_indexes] = rnorm(n=k, mean=100, sd=sd_beta)
sum(which(beta !=0) != sort(nonzero_indexes)) # test that we made the right indexes nonzero
beta = as.matrix(beta)
# x
X = matrix(rnorm(n=n*p, mean=0, sd=5), nrow=n)
# epsilon
E = matrix(rnorm(n=n, mean=0, sd=1), nrow=n)
# y
Y = X%*%beta + E
# note that in the online setting, each t^th row of X and Y is for time t
dim(X)
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
betahats = rep(0, p)
beta_0 = rep(0, p)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
beta_0
g_vec = matrix(nrow=n, ncol=p) # save matrix for the gradients where each gradient g_t is the t^{th} row of the matrix
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
x_t = as.matrix(X[t, ])
t = 1
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
?dot
dot(c(1, 2, 1, 2), c(1, 1, 1, 1))
g_vec[t, ] = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
g_vec[t, ]
dim(g_vec[t, ])
length(g_vec[t, ])
ssum(g_vec[t, ]%*%t(g_vec[t, ]))
sum(g_vec[t, ]%*%t(g_vec[t, ]))
g_t
g_t = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
g_t
g_t = as.matrix(g_t)
G_t = sum(g_t%*%t(g_t))
g_t = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
g_vec[t, ] = g_t
G_t = sum(g_t%*%t(g_t))
betahats[t+1, ] = beta_t - lr*sqrt(1/G_t)*as.matrix(g_t)
lr = 0.0001
betahats[t+1, ] = beta_t - lr*sqrt(1/G_t)*as.matrix(g_t)
betahats[t+1, ]
betahats[t, ]
mean(betahats[t+1, ])
rm(list=ls())
# simulate data: regression
n = 100 # sample size
p = 200 # number of predictors
# beta
k = round(0.05*p, 0) # number of nonzero coefficients
sd_beta = 0.01
nonzero_indexes = sample.int(n=p, size=k)
beta = rep(0, p)
beta[nonzero_indexes] = rnorm(n=k, mean=100, sd=sd_beta)
sum(which(beta !=0) != sort(nonzero_indexes)) # test that we made the right indexes nonzero
beta = as.matrix(beta)
# x
X = matrix(rnorm(n=n*p, mean=0, sd=5), nrow=n)
# epsilon
E = matrix(rnorm(n=n, mean=0, sd=1), nrow=n)
# y
Y = X%*%beta + E
# note that in the online setting, each t^th row of X and Y is for time t
# function for adaptive gradient descent (Adagrad)
# X: rows are observations, columns are predictors
# Y: response variable
# lr: global learning rate
# epsilon: noise for nonzero/invertibility
# beta_0: weight initialization
# full: boolean, uses full matrix for G if true, otherwise uses diagonal elements of G
# rho: decay rate for Adadelta
my_adagrad = function(X, Y, lr, epsilon, beta_0, full, rho) {
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
H = list() # list of nxn matrices
g_vec = matrix(nrow=n, ncol=p) # save matrix for the gradients where each gradient g_t is the t^{th} row of the matrix
for (t in 1:(n-1)) {
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
g_t = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
g_vec[t, ] = g_t
G_t = sum(g_t%*%t(g_t))
betahats[t+1, ] = beta_t - lr*sqrt(1/G_t)*as.matrix(g_t)
} # end for
return(betahats)
}
beta_0 = rep(0, nrow(X))
g_t = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
g_vec = matrix(nrow=n, ncol=p) # save matrix for the gradients where each gradient g_t is the t^{th} row of the matrix
t = 1
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
g_t = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
g_t = as.matrix(g_t)
dim(g_t)
g_vec[t, ] = g_t
g_t%*%t(g_t)
dim(g_t%*%t(g_t))
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
g_vec[t, ] = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
g_t = as.matrix(g_vec[t, ])
dim(g_t)
G_t = matrix(data=rep(0, p^2), nrow=p, ncol=p)
dim(G_t)
dim(g_t%*%t(g_t))
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
g_vec = matrix(nrow=n, ncol=p) # save matrix for the gradients where each gradient g_t is the t^{th} row of the matrix
G_t = matrix(data=rep(0, p^2), nrow=p, ncol=p) # matrix that is a cumulative sum
t = 1
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
g_vec[t, ] = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
g_t = as.matrix(g_vec[t, ])
G_t = G_t + g_t%*%t(g_t)
G_t
G
G_t
dim(G_t)
diag(G_t)
dim(diag(G_t))
length(diag(G_t))
?diag
temp = diag(diag(G_t), nrow=p, ncol=p)
dim(temp)
temp
sum(diag(temp) != diag(G_t))
g_vec[t, ] = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
g_t = as.matrix(g_vec[t, ])
G_t = G_t + g_t%*%t(g_t)
diag_G_t = diag(diag(G_t), nrow=p, ncol=p)
diag_G_t
diag_G_t^(-1/2)
diag_G_t
diag(diag_G_t^(-1/2))
betahats[t+1, ] = beta_t - lr*diag(diag(diag_G_t^(-1/2)), nrow=p, ncol=p)
diag(diag(diag_G_t^(-1/2)), nrow=p, ncol=p)
lr = 0.00001
betahats[t+1, ] = beta_t - lr*diag(diag(diag_G_t^(-1/2)), nrow=p, ncol=p)
dim(diag(diag(diag_G_t^(-1/2)), nrow=p, ncol=p))
betahats[t+1, ] = beta_t - lr*as.matrix(diag(diag(diag_G_t^(-1/2)), nrow=p, ncol=p))%*%g_t
dim(as.matrix(diag(diag(diag_G_t^(-1/2)), nrow=p, ncol=p))%*%g_t)
betahats[t+1, ] = beta_t - lr*as.matrix(diag(diag(diag_G_t^(-1/2)), nrow=p, ncol=p))%*%g_t
# function for adaptive gradient descent (Adagrad)
# X: rows are observations, columns are predictors
# Y: response variable
# lr: global learning rate
# epsilon: noise for nonzero/invertibility
# beta_0: weight initialization
# full: boolean, uses full matrix for G if true, otherwise uses diagonal elements of G
my_adagrad = function(X, Y, lr, beta_0, full) {
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
g_vec = matrix(nrow=n, ncol=p) # save matrix for the gradients where each gradient g_t is the t^{th} row of the matrix
G_t = matrix(data=rep(0, p^2), nrow=p, ncol=p) # matrix that is a cumulative sum
for (t in 1:(n-1)) {
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
g_vec[t, ] = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
g_t = as.matrix(g_vec[t, ])
G_t = G_t + g_t%*%t(g_t)
diag_G_t = diag(diag(G_t), nrow=p, ncol=p)
if (full) {
# full
betahats[t+1, ] = beta_t # TODO, maybe use built in functions for negative squre root
} else {
# diagonal
betahats[t+1, ] = beta_t - lr*as.matrix(diag(diag(diag_G_t^(-1/2)), nrow=p, ncol=p))%*%g_t
}
} # end for
return(betahats)
}
beta_0 = rep(0, nrow(X))
betahats = my_adagrad(X=X, Y=Y, lr=0.0000001, beta_0=beta_0, full=F)
plot(betahats[n, ] - beta, main="Differences between last estimate and true beta")
sum(abs(betahats[n, ] - beta) > 0.5*mean(beta[nonzero_indexes])) == k
mean(betahats[n, nonzero_indexes]) - mean(betahats[n, -nonzero_indexes])
temp = matrix(rep(beta, n), nrow=n, ncol=p, byrow=T)
temp2 = betahats - temp
plot(rowMeans(temp2^2), main="Error")
plot(beta[nonzero_indexes], betahats[n, nonzero_indexes], main="K non-zero indexes for estimated and true betas")
plot(beta, main="True betas")
plot(betahats[n, ], main="Estimated betas")
library(expm)
library(expm)
rm(list=ls())
set.seed(6520)
library(expm)
# simulate data: regression
n = 100 # sample size
p = 200 # number of predictors
# beta
k = round(0.05*p, 0) # number of nonzero coefficients
sd_beta = 0.01
nonzero_indexes = sample.int(n=p, size=k)
beta = rep(0, p)
beta[nonzero_indexes] = rnorm(n=k, mean=100, sd=sd_beta)
sum(which(beta !=0) != sort(nonzero_indexes)) # test that we made the right indexes nonzero
beta = as.matrix(beta)
# x
X = matrix(rnorm(n=n*p, mean=0, sd=5), nrow=n)
# epsilon
E = matrix(rnorm(n=n, mean=0, sd=1), nrow=n)
# y
Y = X%*%beta + E
# note that in the online setting, each t^th row of X and Y is for time t
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
beta_0=rep(0, p)
betahats[1, ] = beta_0
g_vec = matrix(nrow=n, ncol=p) # save matrix for the gradients where each gradient g_t is the t^{th} row of the matrix
G_t = matrix(data=rep(0, p^2), nrow=p, ncol=p) # matrix that is a cumulative sum
t = 1
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
g_vec[t, ] = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
g_t = as.matrix(g_vec[t, ])
G_t = G_t + g_t%*%t(g_t)
dim(as.matrix(solve(sqrtm(G_t))))
dim(as.matrix(solve(sqrtm(G_t)))%*%g_t)
dim(beta_t)
# function for adaptive gradient descent (Adagrad)
# X: rows are observations, columns are predictors
# Y: response variable
# lr: global learning rate
# epsilon: noise for nonzero/invertibility
# beta_0: weight initialization
# full: boolean, uses full matrix for G if true, otherwise uses diagonal elements of G
my_adagrad = function(X, Y, lr, beta_0, full) {
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
g_vec = matrix(nrow=n, ncol=p) # save matrix for the gradients where each gradient g_t is the t^{th} row of the matrix
G_t = matrix(data=rep(0, p^2), nrow=p, ncol=p) # matrix that is a cumulative sum
for (t in 1:(n-1)) {
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
g_vec[t, ] = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
g_t = as.matrix(g_vec[t, ])
G_t = G_t + g_t%*%t(g_t)
diag_G_t = diag(diag(G_t), nrow=p, ncol=p)
if (full) {
# full
betahats[t+1, ] = beta_t - lr*as.matrix(solve(sqrtm(G_t)))%*%g_t
} else {
# diagonal
betahats[t+1, ] = beta_t - lr*as.matrix(diag(diag(diag_G_t^(-1/2)), nrow=p, ncol=p))%*%g_t
}
} # end for
return(betahats)
}
beta_0 = rep(0, nrow(X))
betahats = my_adagrad(X=X, Y=Y, lr=0.0000001, beta_0=beta_0, full=F)
# plotting
plot(betahats[n, ] - beta, main="Differences between last estimate and true beta")
sum(abs(betahats[n, ] - beta) > 0.5*mean(beta[nonzero_indexes])) == k
mean(betahats[n, nonzero_indexes]) - mean(betahats[n, -nonzero_indexes])
temp = matrix(rep(beta, n), nrow=n, ncol=p, byrow=T)
temp2 = betahats - temp
plot(rowMeans(temp2^2), main="Error")
plot(beta[nonzero_indexes], betahats[n, nonzero_indexes], main="K non-zero indexes for estimated and true betas")
plot(beta, main="True betas")
plot(betahats[n, ], main="Estimated betas")
devtools::document()
getwd()
setwd("/Users/jwz34/Documents/Github/6520project/onlinegrad/R/my_OGD.R")
setwd()
setwd("/Users/jwz34/Documents/Github/6520project/onlinegrad/R")
devtools::document()
devtools::install()
library(onlinegrad)
?my_adagrad
devtools::document()
library(onlinegrad)
?my_adagrad
?my_adagrad
betahats_ogd = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p))
betahats_adagrad = my_adagrad(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p), full=F)
betahats_ogd = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p))
betahats_adagrad = my_adagrad(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p), full=F)
dim(betahats_ogd)
n
p
dim(X)
dim(X%*%t(betahats_ogd))
n
Y
dim(Y)
?cbind
?matrix
matrix(rep(Y, n), nrow=n, ncol=n, byrow=F)
sum(matrix(rep(Y, n), nrow=n, ncol=n, byrow=F)[56] != Y)
sum(matrix(rep(Y, n), nrow=n, ncol=n, byrow=F)[, 56] != Y)
dim(X%*%t(betahats_ogd) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))
?colsum
columnsum
?rowsum
?colsum
colSums()
?colSums
# plot prediction error vs iterations
pred_err = colSums((X%*%t(betahats_ogd) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))^2) # row of the inside matrix is observation, column is iteration
dim(pred_err)
colSums
length(pred_err)
colSums
pred_err
var(pred_err)
mean(pred_err)
plot(pred_err)
# plot prediction error vs iterations
pred_err = colSums((X%*%t(betahats_ogd) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))^2) # row of the inside matrix is observation, column is iteration
plot(pred_err, xlab="Iteration", ylab="Prediction error", main="Online gradient descent (OGD)")
pred_err = colSums((X%*%t(betahats_adagrad) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))^2) # row of the inside matrix is observation, column is iteration
plot(pred_err, xlab="Iteration", ylab="Prediction error", main="Adaptive gradient descent (Adagrad)")
# plot prediction error vs iterations
pred_err = colSums((X%*%t(betahats_ogd) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))^2) # row of the inside matrix is observation, column is iteration
plot(pred_err, xlab="Iteration", ylab="Prediction error", main="Online gradient descent (OGD)")
pred_err = colSums((X%*%t(betahats_adagrad) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))^2) # row of the inside matrix is observation, column is iteration
plot(pred_err, xlab="Iteration", ylab="Prediction error", main="Adaptive gradient descent (Adagrad)")
# plot prediction error vs iterations
pred_err = colSums((X%*%t(betahats_ogd) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))^2) # row of the inside matrix is observation, column is iteration
plot(pred_err, xlab="Iteration", ylab="Prediction error", main="Online gradient descent (OGD)")
?my_adagrad
