rm(list=ls())
# simulate data: regression
n = 100 # sample size
p = 200 # number of predictors
# beta
k = round(0.05*p, 0) # number of nonzero coefficients
sd_beta = 0.01
nonzero_indexes = sample.int(n=p, size=k)
beta = rep(0, p)
beta[nonzero_indexes] = rnorm(n=k, mean=100, sd=sd_beta)
sum(which(beta !=0) != sort(nonzero_indexes)) # test that we made the right indexes nonzero
beta = as.matrix(beta)
# x
X = matrix(rnorm(n=n*p, mean=0, sd=5), nrow=n)
# epsilon
E = matrix(rnorm(n=n, mean=0, sd=1), nrow=n)
# y
Y = X%*%beta + E
# note that in the online setting, each t^th row of X and Y is for time t
# function for adaptive gradient descent (Adagrad)
# X: rows are observations, columns are predictors
# Y: response variable
# lr: global learning rate
# epsilon: noise for nonzero/invertibility
# beta_0: weight initialization
# full: boolean, uses full matrix for G if true, otherwise uses diagonal elements of G
# rho: decay rate for Adadelta
my_adagrad = function(X, Y, lr, epsilon, beta_0, full, rho) {
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
H = list() # list of nxn matrices
g_vec = matrix(nrow=n, ncol=p) # save matrix for the gradients where each gradient g_t is the t^{th} row of the matrix
for (t in 1:(n-1)) {
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
g_t = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
g_vec[t, ] = g_t
G_t = sum(g_t%*%t(g_t))
betahats[t+1, ] = beta_t - lr*sqrt(1/G_t)*as.matrix(g_t)
} # end for
return(betahats)
}
beta_0 = rep(0, nrow(X))
g_t = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
g_vec = matrix(nrow=n, ncol=p) # save matrix for the gradients where each gradient g_t is the t^{th} row of the matrix
t = 1
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
g_t = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
g_t = as.matrix(g_t)
dim(g_t)
g_vec[t, ] = g_t
g_t%*%t(g_t)
dim(g_t%*%t(g_t))
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
g_vec[t, ] = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
g_t = as.matrix(g_vec[t, ])
dim(g_t)
G_t = matrix(data=rep(0, p^2), nrow=p, ncol=p)
dim(G_t)
dim(g_t%*%t(g_t))
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
g_vec = matrix(nrow=n, ncol=p) # save matrix for the gradients where each gradient g_t is the t^{th} row of the matrix
G_t = matrix(data=rep(0, p^2), nrow=p, ncol=p) # matrix that is a cumulative sum
t = 1
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
g_vec[t, ] = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
g_t = as.matrix(g_vec[t, ])
G_t = G_t + g_t%*%t(g_t)
G_t
G
G_t
dim(G_t)
diag(G_t)
dim(diag(G_t))
length(diag(G_t))
?diag
temp = diag(diag(G_t), nrow=p, ncol=p)
dim(temp)
temp
sum(diag(temp) != diag(G_t))
g_vec[t, ] = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
g_t = as.matrix(g_vec[t, ])
G_t = G_t + g_t%*%t(g_t)
diag_G_t = diag(diag(G_t), nrow=p, ncol=p)
diag_G_t
diag_G_t^(-1/2)
diag_G_t
diag(diag_G_t^(-1/2))
betahats[t+1, ] = beta_t - lr*diag(diag(diag_G_t^(-1/2)), nrow=p, ncol=p)
diag(diag(diag_G_t^(-1/2)), nrow=p, ncol=p)
lr = 0.00001
betahats[t+1, ] = beta_t - lr*diag(diag(diag_G_t^(-1/2)), nrow=p, ncol=p)
dim(diag(diag(diag_G_t^(-1/2)), nrow=p, ncol=p))
betahats[t+1, ] = beta_t - lr*as.matrix(diag(diag(diag_G_t^(-1/2)), nrow=p, ncol=p))%*%g_t
dim(as.matrix(diag(diag(diag_G_t^(-1/2)), nrow=p, ncol=p))%*%g_t)
betahats[t+1, ] = beta_t - lr*as.matrix(diag(diag(diag_G_t^(-1/2)), nrow=p, ncol=p))%*%g_t
# function for adaptive gradient descent (Adagrad)
# X: rows are observations, columns are predictors
# Y: response variable
# lr: global learning rate
# epsilon: noise for nonzero/invertibility
# beta_0: weight initialization
# full: boolean, uses full matrix for G if true, otherwise uses diagonal elements of G
my_adagrad = function(X, Y, lr, beta_0, full) {
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
betahats[1, ] = beta_0
g_vec = matrix(nrow=n, ncol=p) # save matrix for the gradients where each gradient g_t is the t^{th} row of the matrix
G_t = matrix(data=rep(0, p^2), nrow=p, ncol=p) # matrix that is a cumulative sum
for (t in 1:(n-1)) {
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
g_vec[t, ] = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
g_t = as.matrix(g_vec[t, ])
G_t = G_t + g_t%*%t(g_t)
diag_G_t = diag(diag(G_t), nrow=p, ncol=p)
if (full) {
# full
betahats[t+1, ] = beta_t # TODO, maybe use built in functions for negative squre root
} else {
# diagonal
betahats[t+1, ] = beta_t - lr*as.matrix(diag(diag(diag_G_t^(-1/2)), nrow=p, ncol=p))%*%g_t
}
} # end for
return(betahats)
}
beta_0 = rep(0, nrow(X))
betahats = my_adagrad(X=X, Y=Y, lr=0.0000001, beta_0=beta_0, full=F)
plot(betahats[n, ] - beta, main="Differences between last estimate and true beta")
sum(abs(betahats[n, ] - beta) > 0.5*mean(beta[nonzero_indexes])) == k
mean(betahats[n, nonzero_indexes]) - mean(betahats[n, -nonzero_indexes])
temp = matrix(rep(beta, n), nrow=n, ncol=p, byrow=T)
temp2 = betahats - temp
plot(rowMeans(temp2^2), main="Error")
plot(beta[nonzero_indexes], betahats[n, nonzero_indexes], main="K non-zero indexes for estimated and true betas")
plot(beta, main="True betas")
plot(betahats[n, ], main="Estimated betas")
rm(list=ls())
# simulate data: regression
n = 100 # sample size
p = 200 # number of predictors
# beta
k = round(0.05*p, 0) # number of nonzero coefficients
sd_beta = 0.01
nonzero_indexes = sample.int(n=p, size=k)
beta = rep(0, p)
beta[nonzero_indexes] = rnorm(n=k, mean=100, sd=sd_beta)
sum(which(beta !=0) != sort(nonzero_indexes)) # test that we made the right indexes nonzero
beta = as.matrix(beta)
# x
X = matrix(rnorm(n=n*p, mean=0, sd=5), nrow=n)
# epsilon
E = matrix(rnorm(n=n, mean=0, sd=1), nrow=n)
# y
Y = X%*%beta + E
# note that in the online setting, each t^th row of X and Y is for time t
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
Ms = matrix(nrow=n, ncol=p)
Rs = matrix(nrow=n, ncol=p)
Ms
Rs
Ms[1, ] = rep(0, p)
Rs[1, ] = rep(0, p)
Ms
Rs
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
Ms = matrix(nrow=n, ncol=p) # 1st moment estimate
Rs = matrix(nrow=n, ncol=p) # 2nd moment estimate
Mhats = matrix(nrow=n, ncol=p) # 1st moment bias correction
Rhats = matrix(nrow=n, ncol=p) # 2nd moment bias correction
# initialize
betahats[1, ] = beta_0
Ms[1, ] = rep(0, p)
beta_0=rep(0,p)
# initialize
betahats[1, ] = beta_0
Ms[1, ] = rep(0, p)
Rs[1, ] = rep(0, p)
t = 1
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
# g_vec[t, ] = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
# g_t = as.matrix(g_vec[t, ])
# G_t = G_t + g_t%*%t(g_t)
# diag_G_t = diag(diag(G_t), nrow=p, ncol=p)
d_loss = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
d_loss
dim(d_loss)
Ms[t+1] = rho_1*as.matrix(Ms[t, ]) + (1-rho_1)*d_loss*beta_t
rho_1 = 0.9
Ms[t+1] = rho_1*as.matrix(Ms[t, ]) + (1-rho_1)*d_loss*beta_t
Ms[t+1]
dim(as.matrix(Ms[t, ]))
dim(d_loss)
dim(beta_t)
dim((1-rho_1)*d_loss*beta_t)
rho_1*as.matrix(Ms[t, ]) + (1-rho_1)*d_loss*beta_t
dim(Ms[t, ])
dim(as.matrix(Ms[t, ]))
?as.matrix
dim(as.matrix(Ms[t, ], nrow=p))
Ms[t+1] = rho_1*as.matrix(Ms[t, ]) + (1-rho_1)*d_loss*beta_t
Ms[t+1, ] = rho_1*as.matrix(Ms[t, ]) + (1-rho_1)*d_loss*beta_t
Ms[t+1, ]
dim(Ms[t+1, ] )
length(Ms[t+1, ] )
Rs[t+1, ] = rho_2*as.matrix(Rs[t, ]) + (1-rho_2)*d_loss*beta_t^2 # is this elementwise squaring
rho_2=0.999
Rs[t+1, ] = rho_2*as.matrix(Rs[t, ]) + (1-rho_2)*d_loss*beta_t^2 # is this elementwise squaring
betahats[t+1, ] = beta_t - lr*(Mhats[t+1, ]/(sqrt(Rhats[t+1, ]+epsilon))) # update
lr = 0.00001
betahats[t+1, ] = beta_t - lr*(Mhats[t+1, ]/(sqrt(Rhats[t+1, ]+epsilon))) # update
epsilon = 10^(-8)
epsilon
betahats[t+1, ] = beta_t - lr*(Mhats[t+1, ]/(sqrt(Rhats[t+1, ]+epsilon))) # update
betahats[t+1, ]
beta_t
(sqrt(Rhats[t+1, ]+epsilon))
Rhats[t+1, ]
# g_vec[t, ] = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
# g_t = as.matrix(g_vec[t, ])
# G_t = G_t + g_t%*%t(g_t)
# diag_G_t = diag(diag(G_t), nrow=p, ncol=p)
d_loss = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
Ms[t+1, ] = rho_1*as.matrix(Ms[t, ]) + (1-rho_1)*d_loss*beta_t # is it matrix multiplication between d_loss and beta_t, what should the dimensions of M be
Rs[t+1, ] = rho_2*as.matrix(Rs[t, ]) + (1-rho_2)*d_loss*beta_t^2 # is this elementwise squaring
Mhats[t+1, ] = Ms[t, ] / (1-rho_1^t)
Rhats[t+1, ] = Rs[t, ] / (1-rho_2^t)
betahats[t+1, ] = beta_t - lr*(Mhats[t+1, ]/(sqrt(Rhats[t+1, ]+epsilon))) # update
betahats[t+1, ]
# function for adaptive moment estimation (Adam)
# X: rows are observations, columns are predictors
# Y: response variable
# lr: global learning rate (typical choice 0.001)
# beta_0: weight initialization
# epsilon: positive noise for nonzero/invertibility (typical choice 10^{-8})
# rho_1: 1st moment decay rate (typical choice 0.9)
# rho_2: 2nd moment decay rate (typical choice 0.999)
my_adagrad = function(X, Y, lr, beta_0, rho_1, rho_2, epsilon) {
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
Ms = matrix(nrow=n, ncol=p) # 1st moment estimate
Rs = matrix(nrow=n, ncol=p) # 2nd moment estimate
Mhats = matrix(nrow=n, ncol=p) # 1st moment bias correction
Rhats = matrix(nrow=n, ncol=p) # 2nd moment bias correction
# initialize
betahats[1, ] = beta_0
Ms[1, ] = rep(0, p)
Rs[1, ] = rep(0, p)
# g_vec = matrix(nrow=n, ncol=p) # save matrix for the gradients where each gradient g_t is the t^{th} row of the matrix
# G_t = matrix(data=rep(0, p^2), nrow=p, ncol=p) # matrix that is a cumulative sum
for (t in 1:(n-1)) {
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
# g_vec[t, ] = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
# g_t = as.matrix(g_vec[t, ])
# G_t = G_t + g_t%*%t(g_t)
# diag_G_t = diag(diag(G_t), nrow=p, ncol=p)
d_loss = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
Ms[t+1, ] = rho_1*as.matrix(Ms[t, ]) + (1-rho_1)*d_loss*beta_t # is it matrix multiplication between d_loss and beta_t, what should the dimensions of M be
Rs[t+1, ] = rho_2*as.matrix(Rs[t, ]) + (1-rho_2)*d_loss*beta_t^2 # is this elementwise squaring
Mhats[t+1, ] = Ms[t, ] / (1-rho_1^t)
Rhats[t+1, ] = Rs[t, ] / (1-rho_2^t)
betahats[t+1, ] = beta_t - lr*(Mhats[t+1, ]/(sqrt(Rhats[t+1, ]+epsilon))) # update
} # end for
return(betahats)
}
# function for adaptive moment estimation (Adam)
# X: rows are observations, columns are predictors
# Y: response variable
# lr: global learning rate (typical choice 0.001)
# beta_0: weight initialization
# epsilon: positive noise for nonzero/invertibility (typical choice 10^{-8})
# rho_1: 1st moment decay rate (typical choice 0.9)
# rho_2: 2nd moment decay rate (typical choice 0.999)
my_adagrad = function(X, Y, lr, beta_0, rho_1, rho_2, epsilon) {
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
Ms = matrix(nrow=n, ncol=p) # 1st moment estimate
Rs = matrix(nrow=n, ncol=p) # 2nd moment estimate
Mhats = matrix(nrow=n, ncol=p) # 1st moment bias correction
Rhats = matrix(nrow=n, ncol=p) # 2nd moment bias correction
# initialize
betahats[1, ] = beta_0
Ms[1, ] = rep(0, p)
Rs[1, ] = rep(0, p)
# g_vec = matrix(nrow=n, ncol=p) # save matrix for the gradients where each gradient g_t is the t^{th} row of the matrix
# G_t = matrix(data=rep(0, p^2), nrow=p, ncol=p) # matrix that is a cumulative sum
for (t in 1:(n-1)) {
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
# g_vec[t, ] = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
# g_t = as.matrix(g_vec[t, ])
# G_t = G_t + g_t%*%t(g_t)
# diag_G_t = diag(diag(G_t), nrow=p, ncol=p)
d_loss = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
Ms[t+1, ] = rho_1*as.matrix(Ms[t, ]) + (1-rho_1)*d_loss*beta_t # is it matrix multiplication between d_loss and beta_t, what should the dimensions of M be
Rs[t+1, ] = rho_2*as.matrix(Rs[t, ]) + (1-rho_2)*d_loss*beta_t^2 # is this elementwise squaring
Mhats[t+1, ] = Ms[t, ] / (1-rho_1^t)
Rhats[t+1, ] = Rs[t, ] / (1-rho_2^t)
betahats[t+1, ] = beta_t - lr*(Mhats[t+1, ]/(sqrt(Rhats[t+1, ]+epsilon))) # update
} # end for
return(betahats)
}
1e-8
betahats = my_adam(X=X, Y=Y, lr=0.001, beta_0=rep(0, p), rho_1=0.9, rho_2=0.999, epsilon=10^(-8))
# function for adaptive moment estimation (Adam)
# X: rows are observations, columns are predictors
# Y: response variable
# lr: global learning rate (typical choice 0.001)
# beta_0: weight initialization
# epsilon: positive noise for nonzero/invertibility (typical choice 10^{-8})
# rho_1: 1st moment decay rate (typical choice 0.9)
# rho_2: 2nd moment decay rate (typical choice 0.999)
my_adam = function(X, Y, lr, beta_0, rho_1, rho_2, epsilon) {
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
Ms = matrix(nrow=n, ncol=p) # 1st moment estimate
Rs = matrix(nrow=n, ncol=p) # 2nd moment estimate
Mhats = matrix(nrow=n, ncol=p) # 1st moment bias correction
Rhats = matrix(nrow=n, ncol=p) # 2nd moment bias correction
# initialize
betahats[1, ] = beta_0
Ms[1, ] = rep(0, p)
Rs[1, ] = rep(0, p)
# g_vec = matrix(nrow=n, ncol=p) # save matrix for the gradients where each gradient g_t is the t^{th} row of the matrix
# G_t = matrix(data=rep(0, p^2), nrow=p, ncol=p) # matrix that is a cumulative sum
for (t in 1:(n-1)) {
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
# g_vec[t, ] = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
# g_t = as.matrix(g_vec[t, ])
# G_t = G_t + g_t%*%t(g_t)
# diag_G_t = diag(diag(G_t), nrow=p, ncol=p)
d_loss = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
Ms[t+1, ] = rho_1*as.matrix(Ms[t, ]) + (1-rho_1)*d_loss*beta_t # is it matrix multiplication between d_loss and beta_t, what should the dimensions of M be
Rs[t+1, ] = rho_2*as.matrix(Rs[t, ]) + (1-rho_2)*d_loss*beta_t^2 # is this elementwise squaring
Mhats[t+1, ] = Ms[t, ] / (1-rho_1^t)
Rhats[t+1, ] = Rs[t, ] / (1-rho_2^t)
betahats[t+1, ] = beta_t - lr*(Mhats[t+1, ]/(sqrt(Rhats[t+1, ]+epsilon))) # update
} # end for
return(betahats)
}
betahats = my_adam(X=X, Y=Y, lr=0.001, beta_0=rep(0, p), rho_1=0.9, rho_2=0.999, epsilon=10^(-8))
betahats
sum(betahats)
betahats = my_adam(X=X, Y=Y, lr=0.001, beta_0=runif(p), rho_1=0.9, rho_2=0.999, epsilon=10^(-8))
betahats = my_adam(X=X, Y=Y, lr=0.001, beta_0=runif(p), rho_1=0.9, rho_2=0.999, epsilon=10^(-8))
betahats
View(betahats)
betahats = my_adam(X=X, Y=Y, lr=0.001, beta_0=rep(1, p), rho_1=0.9, rho_2=0.999, epsilon=10^(-8))
betahats = my_adam(X=X, Y=Y, lr=0.001, beta_0=rep(1, p), rho_1=0.9, rho_2=0.999, epsilon=10^(-8))
betahats = my_adam(X=X, Y=Y, lr=0.001, beta_0=rep(.1, p), rho_1=0.9, rho_2=0.999, epsilon=10^(-8))
betahats = my_adam(X=X, Y=Y, lr=0.001, beta_0=runif(p), rho_1=0.9, rho_2=0.999, epsilon=10^(-8))
betahats = my_adam(X=X, Y=Y, lr=0.001, beta_0=runif(p), rho_1=0.9, rho_2=0.999, epsilon=10^(-8))
# function for adaptive moment estimation (Adam)
# X: rows are observations, columns are predictors
# Y: response variable
# lr: global learning rate (typical choice 0.001)
# beta_0: weight initialization
# epsilon: positive noise for nonzero/invertibility (typical choice 10^{-8})
# rho_1: 1st moment decay rate (typical choice 0.9)
# rho_2: 2nd moment decay rate (typical choice 0.999)
my_adam = function(X, Y, lr, beta_0, rho_1, rho_2, epsilon) {
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
Ms = matrix(nrow=n, ncol=p) # 1st moment estimate
Rs = matrix(nrow=n, ncol=p) # 2nd moment estimate
Mhats = matrix(nrow=n, ncol=p) # 1st moment bias correction
Rhats = matrix(nrow=n, ncol=p) # 2nd moment bias correction
# initialize
betahats[1, ] = beta_0
Ms[1, ] = rep(0, p)
Rs[1, ] = rep(0, p)
for (t in 1:(n-1)) {
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
d_loss = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
Ms[t+1, ] = rho_1*as.matrix(Ms[t, ]) + (1-rho_1)*d_loss*beta_t # TODO: is it matrix multiplication between d_loss and beta_t, what should the dimensions of M be
Rs[t+1, ] = rho_2*as.matrix(Rs[t, ]) + (1-rho_2)*d_loss*beta_t^2 # TODO: is this elementwise squaring
Mhats[t+1, ] = Ms[t, ] / (1-rho_1^t)
Rhats[t+1, ] = Rs[t, ] / (1-rho_2^t)
betahats[t+1, ] = beta_t - lr*(Mhats[t+1, ]/(sqrt(Rhats[t+1, ]+epsilon))) # update
} # end for
return(betahats)
}
betahats = my_adam(X=X, Y=Y, lr=0.001, beta_0=runif(p), rho_1=0.9, rho_2=0.999, epsilon=10^(-8)) # TODO: why are there NAs produced
betahats
View(betahats)
betahats = my_adam(X=X, Y=Y, lr=0.001, beta_0=runif(p), rho_1=0.9, rho_2=0.999, epsilon=10^(-8)) # TODO: why are there NAs produced
plot(betahats[n, ] - beta, main="Differences between last estimate and true beta")
sum(abs(betahats[n, ] - beta) > 0.5*mean(beta[nonzero_indexes])) == k
mean(betahats[n, nonzero_indexes]) - mean(betahats[n, -nonzero_indexes])
temp = matrix(rep(beta, n), nrow=n, ncol=p, byrow=T)
temp2 = betahats - temp
plot(rowMeans(temp2^2), main="Error")
plot(beta[nonzero_indexes], betahats[n, nonzero_indexes], main="K non-zero indexes for estimated and true betas")
plot(beta, main="True betas")
plot(betahats[n, ], main="Estimated betas")
betahats = my_adam(X=X, Y=Y, lr=0.001, beta_0=runif(p), rho_1=0.9, rho_2=0.999, epsilon=10^(-8)) # TODO: why are there NAs produced
plot(betahats[n, ] - beta, main="Differences between last estimate and true beta")
sum(abs(betahats[n, ] - beta) > 0.5*mean(beta[nonzero_indexes])) == k
mean(betahats[n, nonzero_indexes]) - mean(betahats[n, -nonzero_indexes])
temp = matrix(rep(beta, n), nrow=n, ncol=p, byrow=T)
temp2 = betahats - temp
plot(rowMeans(temp2^2), main="Error")
plot(beta[nonzero_indexes], betahats[n, nonzero_indexes], main="K non-zero indexes for estimated and true betas")
plot(betahats[n, ] - beta, main="Differences between last estimate and true beta")
sum(abs(betahats[n, ] - beta) > 0.5*mean(beta[nonzero_indexes])) == k
mean(betahats[n, nonzero_indexes]) - mean(betahats[n, -nonzero_indexes])
temp = matrix(rep(beta, n), nrow=n, ncol=p, byrow=T)
temp2 = betahats - temp
plot(rowMeans(temp2^2), main="Error")
plot(beta[nonzero_indexes], betahats[n, nonzero_indexes], main="K non-zero indexes for estimated and true betas")
plot(beta, main="True betas")
plot(betahats[n, ], main="Estimated betas")
# function for adaptive moment estimation (Adam)
# X: rows are observations, columns are predictors
# Y: response variable
# lr: global learning rate (typical choice 0.001)
# beta_0: weight initialization
# epsilon: positive noise for nonzero/invertibility (typical choice 10^{-8})
# rho_1: 1st moment decay rate (typical choice 0.9)
# rho_2: 2nd moment decay rate (typical choice 0.999)
my_adam = function(X, Y, lr, beta_0, rho_1, rho_2, epsilon) {
n = nrow(X)
p = ncol(X)
betahats = matrix(nrow=n, ncol=p)
Ms = matrix(nrow=n, ncol=p) # 1st moment estimate
Rs = matrix(nrow=n, ncol=p) # 2nd moment estimate
Mhats = matrix(nrow=n, ncol=p) # 1st moment bias correction
Rhats = matrix(nrow=n, ncol=p) # 2nd moment bias correction
# initialize
betahats[1, ] = beta_0
Ms[1, ] = rep(0, p)
Rs[1, ] = rep(0, p)
for (t in 1:(n-1)) {
x_t = as.matrix(X[t, ])
beta_t = as.matrix(betahats[t, ])
y_t_hat = t(beta_t)%*%x_t
Y_t = Y[t]
d_loss = 2*beta_t%*%t(x_t)%*%x_t - 2*x_t%*%Y_t
Ms[t+1, ] = rho_1*as.matrix(Ms[t, ]) + (1-rho_1)*d_loss
Rs[t+1, ] = rho_2*as.matrix(Rs[t, ]) + (1-rho_2)*d_loss^2
Mhats[t+1, ] = Ms[t, ] / (1-rho_1^t)
Rhats[t+1, ] = Rs[t, ] / (1-rho_2^t)
betahats[t+1, ] = beta_t - lr*(Mhats[t+1, ]/(sqrt(Rhats[t+1, ]+epsilon))) # update
} # end for
return(betahats)
}
betahats = my_adam(X=X, Y=Y, lr=0.001, beta_0=runif(p), rho_1=0.9, rho_2=0.999, epsilon=10^(-8)) # TODO: why are there NAs produced
betahats
plot(betahats[n, ] - beta, main="Differences between last estimate and true beta")
sum(abs(betahats[n, ] - beta) > 0.5*mean(beta[nonzero_indexes])) == k
mean(betahats[n, nonzero_indexes]) - mean(betahats[n, -nonzero_indexes])
temp = matrix(rep(beta, n), nrow=n, ncol=p, byrow=T)
temp2 = betahats - temp
plot(rowMeans(temp2^2), main="Error")
plot(beta[nonzero_indexes], betahats[n, nonzero_indexes], main="K non-zero indexes for estimated and true betas")
plot(beta, main="True betas")
plot(betahats[n, ], main="Estimated betas")
betahats = my_adam(X=X, Y=Y, lr=0.001, beta_0=runif(p), rho_1=0.9, rho_2=0.999, epsilon=10^(-8))
plot(betahats[n, ] - beta, main="Differences between last estimate and true beta")
sum(abs(betahats[n, ] - beta) > 0.5*mean(beta[nonzero_indexes])) == k
mean(betahats[n, nonzero_indexes]) - mean(betahats[n, -nonzero_indexes])
temp = matrix(rep(beta, n), nrow=n, ncol=p, byrow=T)
temp2 = betahats - temp
plot(rowMeans(temp2^2), main="Error")
plot(beta[nonzero_indexes], betahats[n, nonzero_indexes], main="K non-zero indexes for estimated and true betas")
plot(beta, main="True betas")
plot(betahats[n, ], main="Estimated betas")
