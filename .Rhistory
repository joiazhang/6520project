# simulate data: regression
n = 100 # sample size
p = 200 # number of predictors
# beta
k = round(0.05*p, 0) # number of nonzero coefficients
sd_beta = 0.01
nonzero_indexes = sample.int(n=p, size=k)
beta = rep(0, p)
beta[nonzero_indexes] = rnorm(n=k, mean=100, sd=sd_beta)
sum(which(beta !=0) != sort(nonzero_indexes)) # test that we made the right indexes nonzero
beta = as.matrix(beta)
# x
X = matrix(rnorm(n=n*p, mean=0, sd=5), nrow=n)
# epsilon
E = matrix(rnorm(n=n, mean=0, sd=1), nrow=n)
# y
Y = X%*%beta + E
# note that in the online setting, each t^th row of X and Y is for time t
# plot prediction error
# X: rows are observations, columns are predictors
# Y: response variable
# betahats: n x p matrix where each ith row is the coefficients for the ith iteration and the columns are predictors
# beta: true beta coefficient px1 vector
# title: string for the title of the plot
# type: "prediction" or "estimation" for prediction error or estimation error
plot_prediction_error = function(betahats, beta, X, Y, title, type) {
n = nrow(X)
p = ncol(X)
if ((type!="prediction") && (type!="estimation")) {
stop("type parameter must be 'prediction' or 'estimation'")
}
if (type=="prediction") {
# prediction error
err = colSums((X%*%t(betahats) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))^2) # row of the inside matrix is observation, column is iteration
ylab = "Prediction error"
} else {
# estimation error
beta = t(beta)
beta = matrix(rep(beta, n),nrow=n, byrow=T) # row combine n number of t(beta)'s
err = sqrt(rowSums((betahats - beta)^2)) # TODO: should we have the squareroot of the l2 norm
ylab = "Estimation error"
}
plot(err, xlab="Iteration", ylab=ylab, main=title)
}
# OGD
betahats = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p))
plot_prediction_error(betahats=betahats, X=X, Y=Y, title="Online gradient descent (OGD)", type="prediction")
plot_prediction_error(betahats=betahats, beta=beta, X=X, Y=Y, title="Online gradient descent (OGD)", type="estimation")
plot_prediction_error(betahats=betahats, beta=beta, X=X, Y=Y, title="Online gradient descent (OGD)", type="estimation")
plot_prediction_error(betahats=betahats, X=X, Y=Y, title="Online gradient descent (OGD)", type="prediction")
plot_prediction_error(betahats=betahats, beta=beta, X=X, Y=Y, title="Online gradient descent (OGD)", type="estimation")
plot_prediction_error(betahats=betahats, beta=beta, X=X, Y=Y, title="Online gradient descent (OGD)", type="estimation")
plot_prediction_error(betahats=betahats, X=X, Y=Y, title="Online gradient descent (OGD)", type="prediction")
plot_prediction_error(betahats=betahats, beta=beta, X=X, Y=Y, title="Online gradient descent (OGD)", type="estimation")
plot_prediction_error(betahats=betahats, X=X, Y=Y, title="Online gradient descent (OGD)", type="prediction")
plot_prediction_error(betahats=betahats, beta=beta, X=X, Y=Y, title="Online gradient descent (OGD)", type="estimation")
plot(err, xlab="Iteration", ylab=ylab, main=title)
err
# plot prediction error
# X: rows are observations, columns are predictors
# Y: response variable
# betahats: n x p matrix where each ith row is the coefficients for the ith iteration and the columns are predictors
# beta: true beta coefficient px1 vector
# title: string for the title of the plot
# type: "prediction" or "estimation" for prediction error or estimation error
plot_prediction_error = function(betahats, beta, X, Y, title, type) {
n = nrow(X)
p = ncol(X)
if ((type!="prediction") && (type!="estimation")) {
stop("type parameter must be 'prediction' or 'estimation'")
}
if (type=="prediction") {
# prediction error
err = colSums((X%*%t(betahats) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))^2) # row of the inside matrix is observation, column is iteration
ylab = "Prediction error"
} else {
# estimation error
beta = t(beta)
beta = matrix(rep(beta, n),nrow=n, byrow=T) # row combine n number of t(beta)'s
err = sqrt(rowSums((betahats - beta)^2)) # TODO: should we have the squareroot of the l2 norm
ylab = "Estimation error"
}
plot(err, xlab="Iteration", ylab=ylab, main=title)
}
# OGD
betahats = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p))
plot_prediction_error(betahats=betahats, X=X, Y=Y, title="Online gradient descent (OGD)", type="prediction")
plot_prediction_error(betahats=betahats, beta=beta, X=X, Y=Y, title="Online gradient descent (OGD)", type="estimation")
plot_prediction_error(betahats=betahats, X=X, Y=Y, title="Online gradient descent (OGD)", type="prediction")
plot_prediction_error(betahats=betahats, beta=beta, X=X, Y=Y, title="Online gradient descent (OGD)", type="estimation")
# prediction error
err = colSums((X%*%t(betahats) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))^2) # row of the inside matrix is observation, column is iteration
dim(err)
length(err)
dim((X%*%t(betahats) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))^2)
(X%*%t(betahats) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))^2
str((X%*%t(betahats) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))^2)
# plot prediction error
# X: rows are observations, columns are predictors
# Y: response variable
# betahats: n x p matrix where each ith row is the coefficients for the ith iteration and the columns are predictors
# beta: true beta coefficient px1 vector
# title: string for the title of the plot
# type: "prediction" or "estimation" for prediction error or estimation error
plot_prediction_error = function(betahats, beta, X, Y, title, type) {
n = nrow(X)
p = ncol(X)
if ((type!="prediction") && (type!="estimation")) {
stop("type parameter must be 'prediction' or 'estimation'")
}
if (type=="prediction") {
# prediction error
err = colSums(as.matrix((X%*%t(betahats) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))^2)) # row of the inside matrix is observation, column is iteration
ylab = "Prediction error"
} else {
# estimation error
beta = t(beta)
beta = matrix(rep(beta, n),nrow=n, byrow=T) # row combine n number of t(beta)'s
err = sqrt(rowSums((betahats - beta)^2)) # TODO: should we have the squareroot of the l2 norm
ylab = "Estimation error"
}
plot(err, xlab="Iteration", ylab=ylab, main=title)
}
betahats = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p))
plot_prediction_error(betahats=betahats, X=X, Y=Y, title="Online gradient descent (OGD)", type="prediction")
plot_prediction_error(betahats=betahats, beta=beta, X=X, Y=Y, title="Online gradient descent (OGD)", type="estimation")
# plot prediction error
# X: rows are observations, columns are predictors
# Y: response variable
# betahats: n x p matrix where each ith row is the coefficients for the ith iteration and the columns are predictors
# beta: true beta coefficient px1 vector
# title: string for the title of the plot
# type: "prediction" or "estimation" for prediction error or estimation error
plot_prediction_error = function(betahats, beta, X, Y, title, type) {
n = nrow(X)
p = ncol(X)
if ((type!="prediction") && (type!="estimation")) {
stop("type parameter must be 'prediction' or 'estimation'")
}
if (type=="prediction") {
# prediction error
err = colSums((X%*%t(betahats) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))^2) # row of the inside matrix is observation, column is iteration
ylab = "Prediction error"
} else {
# estimation error
beta = t(beta)
beta = matrix(rep(beta, n),nrow=n, byrow=T) # row combine n number of t(beta)'s
err = sqrt(rowSums((betahats - beta)^2)) # TODO: should we have the squareroot of the l2 norm
ylab = "Estimation error"
}
plot(err, xlab="Iteration", ylab=ylab, main=title)
}
# OGD
betahats = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p))
plot_prediction_error(betahats=betahats, X=X, Y=Y, title="Online gradient descent (OGD)", type="prediction")
plot_prediction_error(betahats=betahats, beta=beta, X=X, Y=Y, title="Online gradient descent (OGD)", type="estimation")
dim(X%*%t(betahats) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))
?colsums
?colSums
dim((X%*%t(betahats) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))^2)
n
[]
p
# prediction error
err = colSums((X%*%t(betahats) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))^2, m=n) # row of the inside matrix is observation, column is iteration
# prediction error
err = colSums((X%*%t(betahats) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))^2) # row of the inside matrix is observation, column is iteration
ylab = "Prediction error"
if (type=="prediction") {
# prediction error
err = colSums((X%*%t(betahats) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))^2) # row of the inside matrix is observation, column is iteration
ylab = "Prediction error"
} else {
# estimation error
beta = t(beta)
beta = matrix(rep(beta, n),nrow=n, byrow=T) # row combine n number of t(beta)'s
err = sqrt(rowSums((betahats - beta)^2)) # TODO: should we have the squareroot of the l2 norm
ylab = "Estimation error"
}
# OGD
betahats = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p))
plot_prediction_error(betahats=betahats, X=X, Y=Y, title="Online gradient descent (OGD)", type="prediction")
plot_prediction_error(betahats=betahats, beta=beta, X=X, Y=Y, title="Online gradient descent (OGD)", type="estimation")
?rowSums
?rowSums
# plot prediction error
# X: rows are observations, columns are predictors
# Y: response variable
# betahats: n x p matrix where each ith row is the coefficients for the ith iteration and the columns are predictors
# beta: true beta coefficient px1 vector
# title: string for the title of the plot
# type: "prediction" or "estimation" for prediction error or estimation error
plot_prediction_error = function(betahats, beta, X, Y, title, type) {
n = nrow(X)
p = ncol(X)
if ((type!="prediction") && (type!="estimation")) {
stop("type parameter must be 'prediction' or 'estimation'")
}
if (type=="prediction") {
# prediction error
err = colSums(as.matrix((X%*%t(betahats) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))^2)) # row of the inside matrix is observation, column is iteration
ylab = "Prediction error"
} else {
# estimation error
beta = t(beta)
beta = matrix(rep(beta, n),nrow=n, byrow=T) # row combine n number of t(beta)'s
err = sqrt(rowSums(as.matrix((betahats - beta)^2))) # TODO: should we have the squareroot of the l2 norm
ylab = "Estimation error"
}
plot(err, xlab="Iteration", ylab=ylab, main=title)
}
# OGD
betahats = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p))
plot_prediction_error(betahats=betahats, X=X, Y=Y, title="Online gradient descent (OGD)", type="prediction")
plot_prediction_error(betahats=betahats, beta=beta, X=X, Y=Y, title="Online gradient descent (OGD)", type="estimation")
# plot prediction error
# X: rows are observations, columns are predictors
# Y: response variable
# betahats: n x p matrix where each ith row is the coefficients for the ith iteration and the columns are predictors
# beta: true beta coefficient px1 vector
# title: string for the title of the plot
# type: "prediction" or "estimation" for prediction error or estimation error
plot_prediction_error = function(betahats, beta, X, Y, title, type) {
n = nrow(X)
p = ncol(X)
if ((type!="prediction") && (type!="estimation")) {
stop("type parameter must be 'prediction' or 'estimation'")
}
if (type=="prediction") {
# prediction error
err = colSums(as.matrix((X%*%t(betahats) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))^2)) # row of the inside matrix is observation, column is iteration
ylab = "Prediction error"
} else {
# estimation error
beta = t(beta)
beta = matrix(rep(beta, n),nrow=n, byrow=T) # row combine n number of t(beta)'s
err = (rowSums(as.matrix((betahats - beta)^2))) # TODO: should we have the squareroot of the l2 norm
ylab = "Estimation error"
}
plot(err, xlab="Iteration", ylab=ylab, main=title)
}
# OGD
betahats = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p))
plot_prediction_error(betahats=betahats, X=X, Y=Y, title="Online gradient descent (OGD)", type="prediction")
plot_prediction_error(betahats=betahats, beta=beta, X=X, Y=Y, title="Online gradient descent (OGD)", type="estimation")
betahats - beta
dim(betahats - beta)
# plot prediction error
# X: rows are observations, columns are predictors
# Y: response variable
# betahats: n x p matrix where each ith row is the coefficients for the ith iteration and the columns are predictors
# beta: true beta coefficient px1 vector
# title: string for the title of the plot
# type: "prediction" or "estimation" for prediction error or estimation error
plot_prediction_error = function(betahats, beta, X, Y, title, type) {
n = nrow(X)
p = ncol(X)
if ((type!="prediction") && (type!="estimation")) {
stop("type parameter must be 'prediction' or 'estimation'")
}
if (type=="prediction") {
# prediction error
err = colSums((X%*%t(betahats) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))^2) # row of the inside matrix is observation, column is iteration
ylab = "Prediction error"
} else {
# estimation error
beta = t(beta)
beta = matrix(rep(beta, n),nrow=n, byrow=T) # row combine n number of t(beta)'s
err = rowSums((betahats - beta)^2) # TODO: should we have the squareroot of the l2 norm
ylab = "Estimation error"
}
plot(err, xlab="Iteration", ylab=ylab, main=title)
}
# OGD
betahats = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p))
plot_prediction_error(betahats=betahats, X=X, Y=Y, title="Online gradient descent (OGD)", type="prediction")
plot_prediction_error(betahats=betahats, beta=beta, X=X, Y=Y, title="Online gradient descent (OGD)", type="estimation")
# simulate data: regression
n = 100 # sample size
p = 200 # number of predictors
# beta
k = round(0.05*p, 0) # number of nonzero coefficients
sd_beta = 0.01
nonzero_indexes = sample.int(n=p, size=k)
beta = rep(0, p)
beta[nonzero_indexes] = rnorm(n=k, mean=100, sd=sd_beta)
sum(which(beta !=0) != sort(nonzero_indexes)) # test that we made the right indexes nonzero
beta = as.matrix(beta)
# x
X = matrix(rnorm(n=n*p, mean=0, sd=5), nrow=n)
# epsilon
E = matrix(rnorm(n=n, mean=0, sd=1), nrow=n)
# y
Y = X%*%beta + E
# note that in the online setting, each t^th row of X and Y is for time t
beta
dim(beta)
n = nrow(X)
p = ncol(X)
# prediction error
err = colSums((X%*%t(betahats) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))^2) # row of the inside matrix is observation, column is iteration
ylab = "Prediction error"
plot(err, xlab="Iteration", ylab=ylab, main=title)
# estimation error
beta = t(beta)
beta = matrix(rep(beta, n),nrow=n, byrow=T) # row combine n number of t(beta)'s
err = rowSums((betahats - beta)^2) # TODO: should we have the squareroot of the l2 norm
ylab = "Estimation error"
plot(err, xlab="Iteration", ylab=ylab, main=title)
# plot prediction error
# X: rows are observations, columns are predictors
# Y: response variable
# betahats: n x p matrix where each ith row is the coefficients for the ith iteration and the columns are predictors
# beta: true beta coefficient px1 vector
# title: string for the title of the plot
# type: "prediction" or "estimation" for prediction error or estimation error
plot_prediction_error = function(betahats, beta, X, Y, title, type) {
n = nrow(X)
p = ncol(X)
# if ((type!="prediction") && (type!="estimation")) {
#   stop("type parameter must be 'prediction' or 'estimation'")
# }
if (type=="prediction") {
# prediction error
err = colSums((X%*%t(betahats) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))^2) # row of the inside matrix is observation, column is iteration
ylab = "Prediction error"
} else {
# estimation error
beta = t(beta)
beta = matrix(rep(beta, n),nrow=n, byrow=T) # row combine n number of t(beta)'s
err = rowSums((betahats - beta)^2) # TODO: should we have the squareroot of the l2 norm
ylab = "Estimation error"
}
plot(err, xlab="Iteration", ylab=ylab, main=title)
}
# OGD
betahats = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p))
plot_prediction_error(betahats=betahats, X=X, Y=Y, title="Online gradient descent (OGD)", type="prediction")
plot_prediction_error(betahats=betahats, beta=beta, X=X, Y=Y, title="Online gradient descent (OGD)", type="estimation")
# OGD
betahats = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p))
plot_prediction_error(betahats=betahats, beta=beta, X=X, Y=Y, title="Online gradient descent (OGD)", type="prediction")
plot_prediction_error(betahats=betahats, beta=beta, X=X, Y=Y, title="Online gradient descent (OGD)", type="estimation")
# OGD
betahats = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p))
plot_prediction_error(betahats=betahats, beta=beta, X=X, Y=Y, title="Online gradient descent (OGD)", type="prediction")
plot_prediction_error(betahats=betahats, beta=beta, X=X, Y=Y, title="Online gradient descent (OGD)", type="estimation")
# simulate data: regression
n = 100 # sample size
p = 200 # number of predictors
# beta
k = round(0.05*p, 0) # number of nonzero coefficients
sd_beta = 0.01
nonzero_indexes = sample.int(n=p, size=k)
beta = rep(0, p)
beta[nonzero_indexes] = rnorm(n=k, mean=100, sd=sd_beta)
sum(which(beta !=0) != sort(nonzero_indexes)) # test that we made the right indexes nonzero
beta = as.matrix(beta)
# x
X = matrix(rnorm(n=n*p, mean=0, sd=5), nrow=n)
# epsilon
E = matrix(rnorm(n=n, mean=0, sd=1), nrow=n)
# y
Y = X%*%beta + E
# note that in the online setting, each t^th row of X and Y is for time t
dim(beta)
# estimation error
beta = t(beta)
dim(beta)
beta = matrix(rep(beta, n),nrow=n, byrow=T) # row combine n number of t(beta)'s
dim(beta)
dim(betahats)
dim(betahats - beta)
dim((betahats - beta)^2)
rowsum((betahats - beta)^2)
?rowsum
rowSums((betahats - beta)^2)
dim(rowSums((betahats - beta)^2))
length(rowSums((betahats - beta)^2))
dim(beta)
err = rowSums((betahats - beta)^2) # TODO: should we have the squareroot of the l2 norm
# plot prediction error
# X: rows are observations, columns are predictors
# Y: response variable
# betahats: n x p matrix where each ith row is the coefficients for the ith iteration and the columns are predictors
# beta: true beta coefficient px1 vector
# title: string for the title of the plot
# type: "prediction" or "estimation" for prediction error or estimation error
plot_prediction_error = function(betahats, beta, X, Y, title, type) {
n = nrow(X)
p = ncol(X)
# if ((type!="prediction") && (type!="estimation")) {
#   stop("type parameter must be 'prediction' or 'estimation'")
# }
if (type=="prediction") {
# prediction error
err = colSums((X%*%t(betahats) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))^2) # row of the inside matrix is observation, column is iteration
ylab = "Prediction error"
} else {
# estimation error
beta = t(beta)
beta = matrix(rep(beta, n),nrow=n, byrow=T) # row combine n number of t(beta)'s
err = rowSums((betahats - beta)^2) # TODO: should we have the squareroot of the l2 norm
ylab = "Estimation error"
}
plot(as.matrix(err), xlab="Iteration", ylab=ylab, main=title)
}
# OGD
betahats = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p))
plot_prediction_error(betahats=betahats, beta=beta, X=X, Y=Y, title="Online gradient descent (OGD)", type="prediction")
plot_prediction_error(betahats=betahats, beta=beta, X=X, Y=Y, title="Online gradient descent (OGD)", type="estimation")
# plot prediction error
# X: rows are observations, columns are predictors
# Y: response variable
# betahats: n x p matrix where each ith row is the coefficients for the ith iteration and the columns are predictors
# beta: true beta coefficient px1 vector
# title: string for the title of the plot
# type: "prediction" or "estimation" for prediction error or estimation error
plot_prediction_error = function(betahats, beta, X, Y, title, type) {
n = nrow(X)
p = ncol(X)
# if ((type!="prediction") && (type!="estimation")) {
#   stop("type parameter must be 'prediction' or 'estimation'")
# }
if (type=="prediction") {
# prediction error
err = colSums((X%*%t(betahats) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))^2) # row of the inside matrix is observation, column is iteration
ylab = "Prediction error"
} else {
# estimation error
beta = t(beta)
beta = matrix(rep(beta, n),nrow=n, byrow=T) # row combine n number of t(beta)'s
err = colSums((betahats - beta)^2) # TODO: should we have the squareroot of the l2 norm
ylab = "Estimation error"
}
plot(as.matrix(err), xlab="Iteration", ylab=ylab, main=title)
}
# OGD
betahats = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p))
plot_prediction_error(betahats=betahats, beta=beta, X=X, Y=Y, title="Online gradient descent (OGD)", type="prediction")
plot_prediction_error(betahats=betahats, beta=beta, X=X, Y=Y, title="Online gradient descent (OGD)", type="estimation")
# plot prediction error
# X: rows are observations, columns are predictors
# Y: response variable
# betahats: n x p matrix where each ith row is the coefficients for the ith iteration and the columns are predictors
# beta: true beta coefficient px1 vector
# title: string for the title of the plot
# type: "prediction" or "estimation" for prediction error or estimation error
plot_prediction_error = function(betahats, beta, X, Y, title, type) {
n = nrow(X)
p = ncol(X)
# if ((type!="prediction") && (type!="estimation")) {
#   stop("type parameter must be 'prediction' or 'estimation'")
# }
if (type=="prediction") {
# prediction error
err = colSums((X%*%t(betahats) - matrix(rep(Y, n), nrow=n, ncol=n, byrow=F))^2) # row of the inside matrix is observation, column is iteration
ylab = "Prediction error"
} else {
# estimation error
beta = t(beta)
beta = matrix(rep(beta, n),nrow=n, byrow=T) # row combine n number of t(beta)'s
err = rowSums((betahats - beta)^2) # TODO: should we have the squareroot of the l2 norm
ylab = "Estimation error"
}
plot(as.matrix(err), xlab="Iteration", ylab=ylab, main=title)
}
# OGD
betahats = my_OGD(X=X, Y=Y, lr=0.0000001, beta_0=rep(0, p))
plot_prediction_error(betahats=betahats, beta=beta, X=X, Y=Y, title="Online gradient descent (OGD)", type="prediction")
plot_prediction_error(betahats=betahats, beta=beta, X=X, Y=Y, title="Online gradient descent (OGD)", type="estimation")
plot(as.matrix(err), xlab="Iteration", ylab=ylab, main=title)
dim(err)
length(err)
dim(probs)
# simulate data: classification
# X, beta same as above
probs = 1/(1+exp(-X%*%beta))
rm(list=ls())
set.seed(6520)
library(ggplot2)
library(expm) # for sqrtm
library(devtools)
library(roxygen2)
# run in console
# setwd("/Users/jwz34/Documents/Github/6520project/onlinegrad")
# devtools::install()
library(onlinegrad)
# .rs.restartR() # if ".rdb is corrupt"
# simulate data: regression
n = 100 # sample size
p = 200 # number of predictors
# beta
k = round(0.05*p, 0) # number of nonzero coefficients
sd_beta = 0.01
nonzero_indexes = sample.int(n=p, size=k)
beta = rep(0, p)
beta[nonzero_indexes] = rnorm(n=k, mean=100, sd=sd_beta)
sum(which(beta !=0) != sort(nonzero_indexes)) # test that we made the right indexes nonzero
beta = as.matrix(beta)
# x
X = matrix(rnorm(n=n*p, mean=0, sd=5), nrow=n)
# epsilon
E = matrix(rnorm(n=n, mean=0, sd=1), nrow=n)
# y
Y = X%*%beta + E
# note that in the online setting, each t^th row of X and Y is for time t
# simulate data: classification
# X, beta same as above
probs = 1/(1+exp(-X%*%beta))
Y = rbinom(n=n, size=1, prob = probs) # Bernoulli
dim(probs)
